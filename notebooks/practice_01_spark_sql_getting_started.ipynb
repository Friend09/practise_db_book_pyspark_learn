{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cb2a7d",
   "metadata": {},
   "source": [
    "# Getting Started with Spark SQL\n",
    "\n",
    "In this practice notebook, we'll learn the basics of working with Spark SQL. Follow the instructions in each section and fill in the code cells with your solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a3b8a",
   "metadata": {},
   "source": [
    "## 1. SparkSession Initialization\n",
    "\n",
    "The **SparkSession** is the entry point to all Spark functionality. It provides a unified interface for working with Spark SQL, DataFrames, and Datasets.\n",
    "\n",
    "### Key Points:\n",
    "- SparkSession replaces the older SparkContext + SQLContext pattern\n",
    "- Use `SparkSession.builder` to create a session\n",
    "- Configure application name and options during creation\n",
    "- Built-in support for Hive features (HiveQL, UDFs, Hive tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe56b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.0\n",
      "Application Name: Spark SQL Getting Started\n",
      "Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark SQL Getting Started\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.conf.get('spark.app.name')}\")\n",
    "print(f\"Master: {spark.conf.get('spark.master')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b9696",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames from Python Data\n",
    "\n",
    "DataFrames can be created from various Python data structures like lists, tuples, and dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483fcec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df_from_tuples = spark.createDataFrame(data, columns)\n",
    "df_from_tuples.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3ad147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------+\n",
      "|age|         city|   name|\n",
      "+---+-------------+-------+\n",
      "| 25|     New York|  Alice|\n",
      "| 30|San Francisco|    Bob|\n",
      "| 30|San Francisco|Charlie|\n",
      "+---+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict = [\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 30, \"city\": \"San Francisco\"},\n",
    "    {\"name\": \"Charlie\", \"age\": 30, \"city\": \"San Francisco\"},\n",
    "]\n",
    "\n",
    "df_from_dict = spark.createDataFrame(data_dict)\n",
    "df_from_dict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e8ec155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_tuples.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536cd8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_dict.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b180ee1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_dict.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11776c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'city', 'name']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_dict.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231bef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88b8bf5d",
   "metadata": {},
   "source": [
    "## 3. Creating DataFrames from Files\n",
    "\n",
    "Spark can read data from various file formats including JSON, CSV, and Parquet. Let's create sample files and read them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab9eb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "# Sample data\n",
    "sample_data = [\n",
    "    {\"name\": \"Michael\", \"age\": None},\n",
    "    {\"name\": \"Andy\", \"age\": 30},\n",
    "    {\"name\": \"Justin\", \"age\": 19},\n",
    "]\n",
    "\n",
    "with open(\"../data/people.json\", \"w\") as f:\n",
    "    for record in sample_data:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "pd.DataFrame(sample_data).to_csv(\"../data/people.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b490e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.json(\"../data/people.json\")\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6654e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|NULL|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../data/people.csv\")\n",
    ")\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34c8f24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa360ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019f32e",
   "metadata": {},
   "source": [
    "## 4. Basic DataFrame Operations\n",
    "\n",
    "Now let's explore fundamental DataFrame operations including selections, filtering, and transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0951ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_json\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb867662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86ec5175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     NULL|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"name\"], df[\"age\"] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37af8808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"age\"] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "321fc097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|NULL|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df[\"age\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f994bf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+\n",
      "| age|   name|is_adult|\n",
      "+----+-------+--------+\n",
      "|NULL|Michael|    NULL|\n",
      "|  30|   Andy|    true|\n",
      "|  19| Justin|    true|\n",
      "+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_adult = df.withColumn(\"is_adult\", df[\"age\"] >= 18)\n",
    "df_with_adult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "650cd9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| age|full_name|\n",
      "+----+---------+\n",
      "|NULL|  Michael|\n",
      "|  30|     Andy|\n",
      "|  19|   Justin|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed = df.withColumnRenamed(\"name\", \"full_name\")\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0289878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  19| Justin|\n",
      "|  30|   Andy|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afacea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|NULL|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df[\"age\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddbfa9",
   "metadata": {},
   "source": [
    "## 5. Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to practice what you've learned.\n",
    "\n",
    "### Exercise 1: Create Your Own DataFrame\n",
    "Create a DataFrame with information about your favorite books including: title, author, year_published, and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4407ddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+------+\n",
      "|      title|        author|year_published|rating|\n",
      "+-----------+--------------+--------------+------+\n",
      "|Tom & Jerry|      Tom Babu|          2013|     1|\n",
      "|    Jinthak|  Jinthak Babu|          2014|     2|\n",
      "|    Chintul|  Chintul Babu|          2010|     3|\n",
      "|     Kifalo|   Kifalo Babu|          2020|     4|\n",
      "|  Chivulalu|Chivulalu Babu|          2005|     5|\n",
      "+-----------+--------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Create your books DataFrame here\n",
    "# TODO: Create a DataFrame with at least 5 books\n",
    "# Include columns: title, author, year_published, rating (1-5)\n",
    "\n",
    "books_data = [\n",
    "    (\"Tom & Jerry\", \"Tom Babu\", 2013, 1),\n",
    "    (\"Jinthak\", \"Jinthak Babu\", 2014, 2),\n",
    "    (\"Chintul\", \"Chintul Babu\", 2010, 3),\n",
    "    (\"Kifalo\", \"Kifalo Babu\", 2020, 4),\n",
    "    (\"Chivulalu\", \"Chivulalu Babu\", 2005, 5),\n",
    "]\n",
    "columns = [\"title\", \"author\", \"year_published\", \"rating\"]\n",
    "\n",
    "# Create DataFrame and show it\n",
    "# df_books = spark.createDataFrame(books_data, [\"title\", \"author\", \"year_published\", \"rating\"])\n",
    "# df_books.show()\n",
    "\n",
    "df_books = spark.createDataFrame(books_data, columns)\n",
    "df_books.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061bd6e4",
   "metadata": {},
   "source": [
    "### Exercise 2: DataFrame Operations\n",
    "Using the books DataFrame you created, perform the following operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = 2024\n",
    "df_books = df_books.withColumn(\n",
    "    \"age_of_book\", lit(current_year) - df_books[\"year_published\"]\n",
    ")\n",
    "df_books.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|      title|rating|\n",
      "+-----------+------+\n",
      "|Tom & Jerry|     1|\n",
      "|    Jinthak|     2|\n",
      "|    Chintul|     3|\n",
      "|     Kifalo|     4|\n",
      "|  Chivulalu|     5|\n",
      "+-----------+------+\n",
      "\n",
      "+---------+--------------+--------------+------+-----------+\n",
      "|    title|        author|year_published|rating|age_of_book|\n",
      "+---------+--------------+--------------+------+-----------+\n",
      "|   Kifalo|   Kifalo Babu|          2020|     4|          4|\n",
      "|Chivulalu|Chivulalu Babu|          2005|     5|         19|\n",
      "+---------+--------------+--------------+------+-----------+\n",
      "\n",
      "+-----------+--------------+--------------+------+-----------+\n",
      "|      title|        author|year_published|rating|age_of_book|\n",
      "+-----------+--------------+--------------+------+-----------+\n",
      "|Tom & Jerry|      Tom Babu|          2013|     1|         12|\n",
      "|    Jinthak|  Jinthak Babu|          2014|     2|         11|\n",
      "|    Chintul|  Chintul Babu|          2010|     3|         15|\n",
      "|     Kifalo|   Kifalo Babu|          2020|     4|          5|\n",
      "|  Chivulalu|Chivulalu Babu|          2005|     5|         20|\n",
      "+-----------+--------------+--------------+------+-----------+\n",
      "\n",
      "+-----------+--------------+--------------+------+-----------+\n",
      "|      title|        author|year_published|rating|age_of_book|\n",
      "+-----------+--------------+--------------+------+-----------+\n",
      "|Tom & Jerry|      Tom Babu|          2013|     1|         11|\n",
      "|    Jinthak|  Jinthak Babu|          2014|     2|         10|\n",
      "|    Chintul|  Chintul Babu|          2010|     3|         14|\n",
      "|     Kifalo|   Kifalo Babu|          2020|     4|          4|\n",
      "|  Chivulalu|Chivulalu Babu|          2005|     5|         19|\n",
      "+-----------+--------------+--------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Exercise 2: DataFrame Operations\n",
    "# TODO: Complete the following operations\n",
    "\n",
    "# 1. Select only title and rating columns\n",
    "df_books.select([\"title\", \"rating\"]).show()\n",
    "\n",
    "# 2. Filter books with rating >= 4\n",
    "df_books.filter(df_books[\"rating\"] >= 4).show()\n",
    "\n",
    "# 3. Add a new column 'age_of_book' (current year - year_published)\n",
    "current_year = 2025\n",
    "df_books.withColumn(\n",
    "    \"age_of_book\", lit(current_year) - df_books[\"year_published\"]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2cfa6f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+------+-----------+\n",
      "|      title|        author|year_published|rating|age_of_book|\n",
      "+-----------+--------------+--------------+------+-----------+\n",
      "|  Chivulalu|Chivulalu Babu|          2005|     5|         19|\n",
      "|     Kifalo|   Kifalo Babu|          2020|     4|          4|\n",
      "|    Chintul|  Chintul Babu|          2010|     3|         14|\n",
      "|    Jinthak|  Jinthak Babu|          2014|     2|         10|\n",
      "|Tom & Jerry|      Tom Babu|          2013|     1|         11|\n",
      "+-----------+--------------+--------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Sort books by rating in descending order\n",
    "df_books.orderBy(df_books[\"rating\"].desc()).show()\n",
    "\n",
    "# 5. Group by author and count the number of books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd3c1e",
   "metadata": {},
   "source": [
    "### Exercise 3: File Operations\n",
    "Create a CSV file with employee data and read it back into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2cf65570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: File Operations\n",
    "# TODO: Complete the following\n",
    "\n",
    "# 1. Create employee data as a list of dictionaries\n",
    "# Include: employee_id, name, department, salary\n",
    "employees = [\n",
    "    {\"employee_id\": 1, \"name\": \"Alice Smith\", \"department\": \"HR\", \"salary\": 60000},\n",
    "    {\n",
    "        \"employee_id\": 2,\n",
    "        \"name\": \"Bob Johnson\",\n",
    "        \"department\": \"Engineering\",\n",
    "        \"salary\": 95000,\n",
    "    },\n",
    "    {\n",
    "        \"employee_id\": 3,\n",
    "        \"name\": \"Charlie Lee\",\n",
    "        \"department\": \"Marketing\",\n",
    "        \"salary\": 70000,\n",
    "    },\n",
    "    {\"employee_id\": 4, \"name\": \"Dana White\", \"department\": \"Finance\", \"salary\": 80000},\n",
    "    {\"employee_id\": 5, \"name\": \"Evan Brown\", \"department\": \"Sales\", \"salary\": 65000},\n",
    "]\n",
    "\n",
    "# 2. Convert to pandas DataFrame and save as CSV\n",
    "df = pd.DataFrame(employees)\n",
    "df.to_csv(\"../data/employees.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a7c0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+------+\n",
      "|employee_id|       name| department|salary|\n",
      "+-----------+-----------+-----------+------+\n",
      "|          1|Alice Smith|         HR| 60000|\n",
      "|          2|Bob Johnson|Engineering| 95000|\n",
      "|          3|Charlie Lee|  Marketing| 70000|\n",
      "|          4| Dana White|    Finance| 80000|\n",
      "|          5| Evan Brown|      Sales| 65000|\n",
      "+-----------+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Read the CSV file back using Spark\n",
    "df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../data/employees.csv\")\n",
    ")\n",
    "\n",
    "# df = spark.read.csv(\"../data/employees.csv\", header=True, sep=\",\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7326c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+------+\n",
      "|employee_id|       name| department|salary|\n",
      "+-----------+-----------+-----------+------+\n",
      "|          1|Alice Smith|         HR| 60000|\n",
      "|          2|Bob Johnson|Engineering| 95000|\n",
      "|          3|Charlie Lee|  Marketing| 70000|\n",
      "|          4| Dana White|    Finance| 80000|\n",
      "|          5| Evan Brown|      Sales| 65000|\n",
      "+-----------+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Display the DataFrame and its schema\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "425820b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c454dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practise_db_book_pyspark_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
