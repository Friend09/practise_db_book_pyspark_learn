{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b788ba1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Schema Specification and RDD Interoperability - Practice Notebook\n",
    "\n",
    "This notebook covers **Interoperating with RDDs** and **Programmatically Specifying Schema** from the [Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand RDD to DataFrame conversion\n",
    "- Learn schema inference vs explicit schema definition\n",
    "- Practice creating DataFrames with custom schemas\n",
    "- Work with StructType and StructField\n",
    "- Handle complex data types\n",
    "\n",
    "## Sections\n",
    "1. **Setup and Basic RDD Operations**\n",
    "2. **Schema Inference from RDDs**\n",
    "3. **Programmatically Specifying Schema**\n",
    "4. **Working with Complex Data Types**\n",
    "5. **Schema Evolution and Validation**\n",
    "6. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/13 18:34:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/13 18:34:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession and SparkContext created successfully!\n",
      "Spark Version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Schema Specification\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"SparkSession and SparkContext created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d600eda",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Basic RDD Operations and DataFrame Conversion\n",
    "\n",
    "First, let's understand how to work with RDDs and convert them to DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from text data (simulating reading from a file)\n",
    "text_data = [\n",
    "    \"Alice,25,Engineer\",\n",
    "    \"Bob,30,Manager\",\n",
    "    \"Charlie,35,Engineer\",\n",
    "    \"Diana,28,Analyst\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c735913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD:\n",
      "['Alice,25,Engineer', 'Bob,30,Manager', 'Charlie,35,Engineer', 'Diana,28,Analyst']\n"
     ]
    }
   ],
   "source": [
    "# Create RDD\n",
    "lines_rdd = sc.parallelize(text_data)\n",
    "print(\"Original RDD:\")\n",
    "print(lines_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9028534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed RDD:\n",
      "[('Alice', 25, 'Engineer'), ('Bob', 30, 'Manager'), ('Charlie', 35, 'Engineer'), ('Diana', 28, 'Analyst')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    return (parts[0], int(parts[1]), parts[2])\n",
    "\n",
    "parsed_rdd = lines_rdd.map(parse_line)\n",
    "print(\"\\nParsed RDD:\")\n",
    "print(parsed_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ab2476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   name|age|     job|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|Engineer|\n",
      "|    Bob| 30| Manager|\n",
      "|Charlie| 35|Engineer|\n",
      "|  Diana| 28| Analyst|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inferred = spark.createDataFrame(parsed_rdd, [\"name\",\"age\",\"job\"])\n",
    "df_inferred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ee935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inferred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df8542",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Programmatically Specifying Schema\n",
    "\n",
    "When schema cannot be inferred or needs to be controlled precisely, we can define it programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom schema definition:\n",
      "StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('job', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "custom_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"job\", StringType(), True)\n",
    "])\n",
    "print(\"Custom schema definition:\")\n",
    "print(custom_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd265ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with custom schema:\n",
      "+-------+---+--------+\n",
      "|   name|age|     job|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|Engineer|\n",
      "|    Bob| 30| Manager|\n",
      "|Charlie| 35|Engineer|\n",
      "|  Diana| 28| Analyst|\n",
      "+-------+---+--------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with custom schema\n",
    "df_custom_schema = spark.createDataFrame(parsed_rdd, custom_schema)\n",
    "print(\"\\nDataFrame with custom schema:\")\n",
    "df_custom_schema.show()\n",
    "df_custom_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63f4803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complex nested schema:\n",
      "struct<employee_id:int,personal_info:struct<name:string,age:int,email:string>,job_details:struct<title:string,salary:double,start_date:date>,skills:array<string>>\n"
     ]
    }
   ],
   "source": [
    "# More complex schema example\n",
    "complex_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), False),  # Not nullable\n",
    "    StructField(\"personal_info\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"email\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"job_details\", StructType([\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"salary\", DoubleType(), True),\n",
    "        StructField(\"start_date\", DateType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "print(\"\\nComplex nested schema:\")\n",
    "print(complex_schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e6045",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Working with Different Data Types\n",
    "\n",
    "Explore various Spark SQL data types and how to use them in schema definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8932c475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='Alice', salary=75000.5, is_active=True, hire_date=datetime.date(2020, 1, 15), last_login=datetime.datetime(2024, 1, 10, 14, 30), bonus=Decimal('5000.25'), skills=['Python', 'SQL', 'Spark'], metadata={'department': 'Engineering', 'level': 'Senior'}),\n",
       " Row(id=2, name='Bob', salary=85000.75, is_active=False, hire_date=datetime.date(2019, 3, 20), last_login=datetime.datetime(2024, 1, 9, 9, 15), bonus=Decimal('7500.00'), skills=['Java', 'Scala', 'Kafka'], metadata={'department': 'Engineering', 'level': 'Lead'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample data with different types\n",
    "sample_data = [\n",
    "    Row(\n",
    "        id=1,\n",
    "        name=\"Alice\",\n",
    "        salary=75000.50,\n",
    "        is_active=True,\n",
    "        hire_date=date(2020, 1, 15),\n",
    "        last_login=datetime(2024, 1, 10, 14, 30, 0),\n",
    "        bonus=Decimal(\"5000.25\"),\n",
    "        skills=[\"Python\", \"SQL\", \"Spark\"],\n",
    "        metadata={\"department\": \"Engineering\", \"level\": \"Senior\"}\n",
    "    ),\n",
    "    Row(\n",
    "        id=2,\n",
    "        name=\"Bob\",\n",
    "        salary=85000.75,\n",
    "        is_active=False,\n",
    "        hire_date=date(2019, 3, 20),\n",
    "        last_login=datetime(2024, 1, 9, 9, 15, 0),\n",
    "        bonus=Decimal(\"7500.00\"),\n",
    "        skills=[\"Java\", \"Scala\", \"Kafka\"],\n",
    "        metadata={\"department\": \"Engineering\", \"level\": \"Lead\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "652be97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with various data types:\n",
      "+---+-----+--------+---------+----------+-------------------+-----------------------+--------------------+--------------------------------------------+\n",
      "|id |name |salary  |is_active|hire_date |last_login         |bonus                  |skills              |metadata                                    |\n",
      "+---+-----+--------+---------+----------+-------------------+-----------------------+--------------------+--------------------------------------------+\n",
      "|1  |Alice|75000.5 |true     |2020-01-15|2024-01-10 14:30:00|5000.250000000000000000|[Python, SQL, Spark]|{department -> Engineering, level -> Senior}|\n",
      "|2  |Bob  |85000.75|false    |2019-03-20|2024-01-09 09:15:00|7500.000000000000000000|[Java, Scala, Kafka]|{department -> Engineering, level -> Lead}  |\n",
      "+---+-----+--------+---------+----------+-------------------+-----------------------+--------------------+--------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- last_login: timestamp (nullable = true)\n",
      " |-- bonus: decimal(38,18) (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- metadata: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame from Row objects (schema inferred)\n",
    "df_various_types = spark.createDataFrame(sample_data)\n",
    "print(\"DataFrame with various data types:\")\n",
    "df_various_types.show(truncate=False)\n",
    "df_various_types.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1427fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explicit schema for various data types:\n",
      "struct<id:int,name:string,salary:double,is_active:boolean,hire_date:date,last_login:timestamp,bonus:decimal(10,2),skills:array<string>,metadata:map<string,string>>\n"
     ]
    }
   ],
   "source": [
    "# Define explicit schema for the same data\n",
    "explicit_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True),\n",
    "    StructField(\"last_login\", TimestampType(), True),\n",
    "    StructField(\"bonus\", DecimalType(10, 2), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True),\n",
    "    StructField(\"metadata\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "print(\"\\nExplicit schema for various data types:\")\n",
    "print(explicit_schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb8f5e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Schema Validation and Error Handling\n",
    "\n",
    "Learn how to handle schema mismatches and validation errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCHEMA VALIDATION ===\n"
     ]
    }
   ],
   "source": [
    "# Schema validation examples\n",
    "print(\"=== SCHEMA VALIDATION ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82a41db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1214989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| id| name| salary|\n",
      "+---+-----+-------+\n",
      "|  1|Alice|75000.0|\n",
      "|  2|  Bob|85000.0|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_data = [(1, \"Alice\", 75000.0), (2, \"Bob\", 85000.0)]\n",
    "df_valid = spark.createDataFrame(valid_data, strict_schema)\n",
    "df_valid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3a6df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [FIELD_NOT_NULLABLE_WITH_NAME] field name: This field is not nullable, but got None.\n"
     ]
    }
   ],
   "source": [
    "# Try to create DataFrame with invalid data (this will work but may cause issues later)\n",
    "try:\n",
    "    invalid_data = [(1, \"Alice\", 75000.0), (2, None, 85000.0)]  # None in non-nullable field\n",
    "    df_invalid = spark.createDataFrame(invalid_data, strict_schema)\n",
    "    print(\"DataFrame created with invalid data:\")\n",
    "    df_invalid.show()  # This might fail or show unexpected results\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "711e774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCHEMA COMPARISON ===\n",
      "Schema 1: struct<id:int,name:string>\n",
      "Schema 2: struct<id:bigint,name:string>\n",
      "Are schemas equal? False\n"
     ]
    }
   ],
   "source": [
    "# Schema comparison\n",
    "print(\"\\n=== SCHEMA COMPARISON ===\")\n",
    "schema1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"id\", LongType(), True),  # Different type\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schema 1:\", schema1.simpleString())\n",
    "print(\"Schema 2:\", schema2.simpleString())\n",
    "print(\"Are schemas equal?\", schema1 == schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a04337ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NULLABLE VS NON-NULLABLE ===\n",
      "Nullable schema: struct<id:int,name:string>\n",
      "Field 'id': nullable=True, type=IntegerType()\n",
      "Field 'name': nullable=False, type=StringType()\n"
     ]
    }
   ],
   "source": [
    "# Working with nullable vs non-nullable fields\n",
    "print(\"\\n=== NULLABLE VS NON-NULLABLE ===\")\n",
    "nullable_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),    # Nullable\n",
    "    StructField(\"name\", StringType(), False)   # Non-nullable\n",
    "])\n",
    "\n",
    "print(\"Nullable schema:\", nullable_schema.simpleString())\n",
    "for field in nullable_schema.fields:\n",
    "    print(f\"Field '{field.name}': nullable={field.nullable}, type={field.dataType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf94ad4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Practice Exercises\n",
    "\n",
    "Complete these exercises to practice schema specification and RDD operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXERCISE 1: Parse RDD and Create DataFrame ===\n",
      "Raw data:\n",
      "1,John Doe,Software Engineer,75000,2020-01-15,Python;Java;SQL\n",
      "2,Jane Smith,Data Scientist,85000,2019-06-20,Python;R;Machine Learning\n",
      "3,Bob Johnson,DevOps Engineer,80000,2021-03-10,Docker;Kubernetes;AWS\n",
      "4,Alice Brown,Product Manager,90000,2018-09-05,Agile;Scrum;Analytics\n",
      "\n",
      "TODO: Parse the RDD and create a DataFrame\n",
      "1. Create a function to parse each line\n",
      "2. Split by comma and handle the skills field (split by semicolon)\n",
      "3. Convert to appropriate data types\n",
      "4. Create DataFrame with inferred schema\n"
     ]
    }
   ],
   "source": [
    "exercise_data = [\n",
    "    \"1,John Doe,Software Engineer,75000,2020-01-15,Python;Java;SQL\",\n",
    "    \"2,Jane Smith,Data Scientist,85000,2019-06-20,Python;R;Machine Learning\",\n",
    "    \"3,Bob Johnson,DevOps Engineer,80000,2021-03-10,Docker;Kubernetes;AWS\",\n",
    "    \"4,Alice Brown,Product Manager,90000,2018-09-05,Agile;Scrum;Analytics\"\n",
    "]\n",
    "\n",
    "# Create RDD from the data\n",
    "exercise_rdd = sc.parallelize(exercise_data)\n",
    "\n",
    "print(\"=== EXERCISE 1: Parse RDD and Create DataFrame ===\")\n",
    "print(\"Raw data:\")\n",
    "for line in exercise_data:\n",
    "    print(line)\n",
    "\n",
    "# TODO: Complete this exercise\n",
    "print(\"\\nTODO: Parse the RDD and create a DataFrame\")\n",
    "print(\"1. Create a function to parse each line\")\n",
    "print(\"2. Split by comma and handle the skills field (split by semicolon)\")\n",
    "print(\"3. Convert to appropriate data types\")\n",
    "print(\"4. Create DataFrame with inferred schema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7144083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
