{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c806f33a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Data Sources and File Formats - Practice Notebook\n",
    "\n",
    "This notebook covers reading and writing data in various formats, exploring Spark's built-in data sources.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Read and write CSV, JSON, Parquet files\n",
    "- Understand file format options and configurations\n",
    "- Work with different data sources\n",
    "- Handle schema evolution and data quality issues\n",
    "- Optimize file formats for performance\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. **CSV Files - Reading and Writing**\n",
    "2. **JSON Files - Handling Semi-structured Data**\n",
    "3. **Parquet Files - Columnar Storage**\n",
    "4. **File Format Comparison**\n",
    "5. **Advanced Data Source Options**\n",
    "6. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/14 09:00:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/14 09:00:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/14 09:00:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/07/14 09:00:05 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/07/14 09:00:05 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/07/14 09:00:05 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+----------+--------------------+\n",
      "| id|   name| department|salary| hire_date|              skills|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|       [Python, SQL]|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|        [CRM, Excel]|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|       [Java, Scala]|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28| [Analytics, Design]|\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|[Negotiation, Pre...|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Data Sources and Formats\").getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "sample_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\", [\"Python\", \"SQL\"]),\n",
    "    (2, \"Bob\", \"Sales\", 65000, \"2019-03-20\", [\"CRM\", \"Excel\"]),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000, \"2018-06-10\", [\"Java\", \"Scala\"]),\n",
    "    (4, \"Diana\", \"Marketing\", 70000, \"2021-02-28\", [\"Analytics\", \"Design\"]),\n",
    "    (5, \"Eve\", \"Sales\", 68000, \"2017-11-05\", [\"Negotiation\", \"Presentation\"]),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    sample_data, [\"id\", \"name\", \"department\", \"salary\", \"hire_date\", \"skills\"]\n",
    ")\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(\"../data/formats\", exist_ok=True)\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb343311",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. CSV Files - Reading and Writing\n",
    "\n",
    "CSV is one of the most common data formats. Let's explore various CSV options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WRITING CSV FILES ===\n"
     ]
    }
   ],
   "source": [
    "# Writing CSV files\n",
    "print(\"=== WRITING CSV FILES ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503a437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+----------+--------------------+\n",
      "| id|   name| department|salary| hire_date|              skills|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|       [Python, SQL]|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|        [CRM, Excel]|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|       [Java, Scala]|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28| [Analytics, Design]|\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|[Negotiation, Pre...|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6db0635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+----------+--------------------+\n",
      "| id|   name| department|salary| hire_date|              skills|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|          Python,SQL|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|           CRM,Excel|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|          Java,Scala|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28|    Analytics,Design|\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|Negotiation,Prese...|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Convert complex types (arrays) to strings before writing to CSV\n",
    "# The 'skills' column is an array, so we need to serialize it as a comma-separated string\n",
    "\n",
    "df_csv_ready = df.withColumn(\"skills\", F.concat_ws(\",\", \"skills\"))\n",
    "df_csv_ready.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec555931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with array column:\n",
      "+-------+---------------------------+\n",
      "|name   |skills                     |\n",
      "+-------+---------------------------+\n",
      "|Alice  |[Python, SQL]              |\n",
      "|Bob    |[CRM, Excel]               |\n",
      "|Charlie|[Java, Scala]              |\n",
      "|Diana  |[Analytics, Design]        |\n",
      "|Eve    |[Negotiation, Presentation]|\n",
      "+-------+---------------------------+\n",
      "\n",
      "DataFrame prepared for CSV (skills as string):\n",
      "+-------+------------------------+\n",
      "|name   |skills                  |\n",
      "+-------+------------------------+\n",
      "|Alice  |Python,SQL              |\n",
      "|Bob    |CRM,Excel               |\n",
      "|Charlie|Java,Scala              |\n",
      "|Diana  |Analytics,Design        |\n",
      "|Eve    |Negotiation,Presentation|\n",
      "+-------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original DataFrame with array column:\")\n",
    "df.select(\"name\", \"skills\").show(truncate=False)\n",
    "print(\"DataFrame prepared for CSV (skills as string):\")\n",
    "df_csv_ready.select(\"name\", \"skills\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f1a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CSV file written (skills column serialized as comma-separated string)\n"
     ]
    }
   ],
   "source": [
    "# Basic CSV write (with skills serialized as string)\n",
    "df_csv_ready.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "    \"../data/formats/employees.csv\"\n",
    ")\n",
    "print(\"Basic CSV file written (skills column serialized as comma-separated string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9872964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom CSV file written (skills column serialized as comma-separated string)\n"
     ]
    }
   ],
   "source": [
    "# CSV with custom options (with skills serialized as string)\n",
    "df_csv_ready.write.mode(\"overwrite\").option(\"header\", \"true\").option(\n",
    "    \"delimiter\", \"|\"\n",
    ").option(\"quote\", '\"').option(\"escape\", \"\\\\\").csv(\n",
    "    \"../data/formats/employees_custom.csv\"\n",
    ")\n",
    "print(\"Custom CSV file written (skills column serialized as comma-separated string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a4ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== READING CSV FILES ===\n"
     ]
    }
   ],
   "source": [
    "# Reading CSV files\n",
    "print(\"\\n=== READING CSV FILES ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "595ad3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CSV read (skills as string):\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "| id|   name| department|salary| hire_date|              skills|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|Negotiation,Prese...|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28|    Analytics,Design|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|          Java,Scala|\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|          Python,SQL|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|           CRM,Excel|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic CSV read\n",
    "df_csv_basic = spark.read.option(\"header\", \"true\").csv(\"../data/formats/employees.csv\")\n",
    "print(\"Basic CSV read (skills as string):\")\n",
    "df_csv_basic.show()\n",
    "df_csv_basic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca83784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skills column converted back to array:\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "| id|   name| department|salary| hire_date|              skills|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|[Negotiation, Pre...|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28| [Analytics, Design]|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|       [Java, Scala]|\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|       [Python, SQL]|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|        [CRM, Excel]|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'skills' string column back to an array if needed\n",
    "df_csv_with_array = df_csv_basic.withColumn(\"skills\", F.split(\"skills\", \",\"))\n",
    "print(\"\\nSkills column converted back to array:\")\n",
    "df_csv_with_array.show()\n",
    "df_csv_with_array.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV read with schema inference:\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "| id|   name| department|salary| hire_date|              skills|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|Negotiation,Prese...|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28|    Analytics,Design|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|          Java,Scala|\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|          Python,SQL|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|           CRM,Excel|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CSV read with schema inference\n",
    "df_csv_infer = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../data/formats/employees.csv\")\n",
    ")\n",
    "print(\"\\nCSV read with schema inference:\")\n",
    "df_csv_infer.show()\n",
    "df_csv_infer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb098553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV read with custom delimiter:\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "| id|   name| department|salary| hire_date|              skills|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|Negotiation,Prese...|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28|    Analytics,Design|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|          Java,Scala|\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|          Python,SQL|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|           CRM,Excel|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CSV read with custom delimiter\n",
    "df_csv_custom = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \"|\")\n",
    "    .csv(\"../data/formats/employees_custom.csv\")\n",
    ")\n",
    "print(\"\\nCSV read with custom delimiter:\")\n",
    "df_csv_custom.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0162c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. JSON Files - Handling Semi-structured Data\n",
    "\n",
    "JSON is perfect for semi-structured data with nested fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WRITING JSON FILES ===\n",
      "JSON file written\n",
      "\n",
      "=== READING JSON FILES ===\n",
      "JSON read:\n",
      "+-----------+----------+---+-------+------+--------------------+\n",
      "| department| hire_date| id|   name|salary|              skills|\n",
      "+-----------+----------+---+-------+------+--------------------+\n",
      "|      Sales|2017-11-05|  5|    Eve| 68000|[Negotiation, Pre...|\n",
      "|  Marketing|2021-02-28|  4|  Diana| 70000| [Analytics, Design]|\n",
      "|Engineering|2018-06-10|  3|Charlie| 80000|       [Java, Scala]|\n",
      "|Engineering|2020-01-15|  1|  Alice| 75000|       [Python, SQL]|\n",
      "|      Sales|2019-03-20|  2|    Bob| 65000|        [CRM, Excel]|\n",
      "+-----------+----------+---+-------+------+--------------------+\n",
      "\n",
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Writing JSON files\n",
    "print(\"=== WRITING JSON FILES ===\")\n",
    "\n",
    "# Basic JSON write\n",
    "df.write.mode(\"overwrite\").json(\"../data/formats/employees.json\")\n",
    "print(\"JSON file written\")\n",
    "\n",
    "# Reading JSON files\n",
    "print(\"\\n=== READING JSON FILES ===\")\n",
    "\n",
    "df_json = spark.read.json(\"../data/formats/employees.json\")\n",
    "print(\"JSON read:\")\n",
    "df_json.show()\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38658ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more complex JSON data\n",
    "complex_data = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"personal\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@company.com\"},\n",
    "        \"job\": {\"title\": \"Engineer\", \"salary\": 75000, \"department\": \"Engineering\"},\n",
    "        \"skills\": [\"Python\", \"SQL\", \"Spark\"],\n",
    "        \"projects\": [\n",
    "            {\"name\": \"Project A\", \"status\": \"completed\"},\n",
    "            {\"name\": \"Project B\", \"status\": \"in_progress\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"personal\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@company.com\"},\n",
    "        \"job\": {\"title\": \"Analyst\", \"salary\": 65000, \"department\": \"Sales\"},\n",
    "        \"skills\": [\"Excel\", \"PowerBI\"],\n",
    "        \"projects\": [{\"name\": \"Project C\", \"status\": \"completed\"}],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a269b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and write\n",
    "df_complex = spark.createDataFrame(complex_data)\n",
    "df_complex.write.mode(\"overwrite\").json(\"../data/formats/employees_complex.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8240dc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complex JSON structure:\n",
      "+---+------------------------------+------------------------------+--------------------------------------------------+--------------------+\n",
      "|id |job                           |personal                      |projects                                          |skills              |\n",
      "+---+------------------------------+------------------------------+--------------------------------------------------+--------------------+\n",
      "|1  |{Engineering, 75000, Engineer}|{30, alice@company.com, Alice}|[{Project A, completed}, {Project B, in_progress}]|[Python, SQL, Spark]|\n",
      "|2  |{Sales, 65000, Analyst}       |{25, bob@company.com, Bob}    |[{Project C, completed}]                          |[Excel, PowerBI]    |\n",
      "+---+------------------------------+------------------------------+--------------------------------------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- job: struct (nullable = true)\n",
      " |    |-- department: string (nullable = true)\n",
      " |    |-- salary: string (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |-- personal: struct (nullable = true)\n",
      " |    |-- age: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- projects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- status: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read complex JSON\n",
    "df_complex_read = spark.read.json(\"../data/formats/employees_complex.json\")\n",
    "print(\"\\nComplex JSON structure:\")\n",
    "df_complex_read.show(truncate=False)\n",
    "df_complex_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accessing nested fields:\n",
      "+---+-----+--------+------+\n",
      "| id| name|   title|salary|\n",
      "+---+-----+--------+------+\n",
      "|  1|Alice|Engineer| 75000|\n",
      "|  2|  Bob| Analyst| 65000|\n",
      "+---+-----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access nested fields\n",
    "print(\"\\nAccessing nested fields:\")\n",
    "df_complex_read.select(\"id\", \"personal.name\", \"job.title\", \"job.salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfeb4ff",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Parquet Files - Columnar Storage\n",
    "\n",
    "Parquet is the preferred format for big data analytics due to its efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WRITING PARQUET FILES ===\n",
      "Parquet file written\n",
      "Partitioned Parquet file written\n",
      "\n",
      "=== READING PARQUET FILES ===\n",
      "Parquet read:\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "| id|   name| department|salary| hire_date|              skills|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|       [Java, Scala]|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28| [Analytics, Design]|\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|       [Python, SQL]|\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|[Negotiation, Pre...|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|        [CRM, Excel]|\n",
      "+---+-------+-----------+------+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Writing Parquet files\n",
    "print(\"=== WRITING PARQUET FILES ===\")\n",
    "\n",
    "# Basic Parquet write\n",
    "df.write.mode(\"overwrite\").parquet(\"../data/formats/employees.parquet\")\n",
    "print(\"Parquet file written\")\n",
    "\n",
    "# Parquet with partitioning\n",
    "df.write.mode(\"overwrite\").partitionBy(\"department\").parquet(\n",
    "    \"../data/formats/employees_partitioned.parquet\"\n",
    ")\n",
    "print(\"Partitioned Parquet file written\")\n",
    "\n",
    "# Reading Parquet files\n",
    "print(\"\\n=== READING PARQUET FILES ===\")\n",
    "\n",
    "df_parquet = spark.read.parquet(\"../data/formats/employees.parquet\")\n",
    "print(\"Parquet read:\")\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7f04013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partitioned Parquet read:\n",
      "+---+-------+------+----------+--------------------+-----------+\n",
      "| id|   name|salary| hire_date|              skills| department|\n",
      "+---+-------+------+----------+--------------------+-----------+\n",
      "|  5|    Eve| 68000|2017-11-05|[Negotiation, Pre...|      Sales|\n",
      "|  4|  Diana| 70000|2021-02-28| [Analytics, Design]|  Marketing|\n",
      "|  3|Charlie| 80000|2018-06-10|       [Java, Scala]|Engineering|\n",
      "|  1|  Alice| 75000|2020-01-15|       [Python, SQL]|Engineering|\n",
      "|  2|    Bob| 65000|2019-03-20|        [CRM, Excel]|      Sales|\n",
      "+---+-------+------+----------+--------------------+-----------+\n",
      "\n",
      "\n",
      "Parquet schema preservation:\n",
      "Original schema: struct<id:bigint,name:string,department:string,salary:bigint,hire_date:string,skills:array<string>>\n",
      "Parquet schema: struct<id:bigint,name:string,department:string,salary:bigint,hire_date:string,skills:array<string>>\n",
      "Schemas match: True\n"
     ]
    }
   ],
   "source": [
    "# Read partitioned Parquet\n",
    "df_partitioned = spark.read.parquet(\"../data/formats/employees_partitioned.parquet\")\n",
    "print(\"\\nPartitioned Parquet read:\")\n",
    "df_partitioned.show()\n",
    "\n",
    "# Parquet preserves schema perfectly\n",
    "print(\"\\nParquet schema preservation:\")\n",
    "print(\"Original schema:\", df.schema.simpleString())\n",
    "print(\"Parquet schema:\", df_parquet.schema.simpleString())\n",
    "print(\"Schemas match:\", df.schema == df_parquet.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfbb5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parquet files with different compression written\n"
     ]
    }
   ],
   "source": [
    "# Parquet with compression\n",
    "df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\n",
    "    \"../data/formats/employees_snappy.parquet\"\n",
    ")\n",
    "df.write.mode(\"overwrite\").option(\"compression\", \"gzip\").parquet(\n",
    "    \"../data/formats/employees_gzip.parquet\"\n",
    ")\n",
    "print(\"\\nParquet files with different compression written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c01b5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Practice Exercises\n",
    "\n",
    "Complete these exercises to practice working with different data formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
