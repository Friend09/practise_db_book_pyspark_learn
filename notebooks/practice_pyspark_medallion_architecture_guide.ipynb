{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# PySpark Medallion Architecture Learning Guide\n",
        "\n",
        "Welcome to your PySpark learning repository! This notebook will guide you through the core concepts of PySpark for data engineering, focusing on the Medallion Architecture (Bronze, Silver, Gold layers). You'll learn how to set up your local environment, ingest raw data, transform it, and aggregate it for analytical purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setting up your Spark Session\n",
        "\n",
        "Before we begin, let's ensure your Spark session is correctly configured. We've provided a utility function in `conf/spark_session_config.py` to help with this. Run the following Python code to initialize your Spark session.\n",
        "\n",
        "**Exercise 1.1**: Run the cell below to create a SparkSession and print its version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.path.dirname(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.path.abspath(os.path.dirname(os.getcwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary modules and create SparkSession with proper configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Bronze Layer: Raw Data Ingestion\n",
        "\n",
        "The Bronze layer is where raw data is ingested as-is from source systems. We've already generated some sample sales data for you in `data/raw/sales_data.csv`. The `scripts/bronze_ingestion.py` script reads this CSV and saves it as a Delta table in `data/bronze/sales`.\n",
        "\n",
        "**Exercise 2.1**: Run the `bronze_ingestion.py` script from your terminal (or a new cell if you prefer, but it's designed to be run as a script). Then, use PySpark to read the ingested Bronze table and display its schema and a few rows.\n",
        "\n",
        "```bash\n",
        "# From your project root directory in the terminal:\n",
        "# PYTHONPATH=$(pwd) python3 scripts/bronze_ingestion.py\n",
        "```\n",
        "\n",
        "**Exercise 2.2**: Read the Bronze table using PySpark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql import functions as F\n",
        "\n",
        "# spark = SparkSession.builder.appName(\"MedallionArchitectureGuide\").getOrCreate()\n",
        "\n",
        "# spark.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"BronzeLayerIngestion\").getOrCreate()\n",
        "\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(\"../data/raw/sales_data.csv\")\n",
        "# df = spark.read.option(\"header\", \"true\").csv(\"../data/formats/employees.csv\")\n",
        "print(\"Basic CSV read (skills as string):\")\n",
        "df.limit(5).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.write.mode(\"overwrite\").parquet(\"../data/bronze/sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Silver Layer: Cleaned and Conformed Data\n",
        "\n",
        "The Silver layer contains cleaned, conformed, and enriched data. In our example, we'll calculate the `total_price` and add a `processing_timestamp`. The `scripts/silver_transformation.py` script performs this transformation.\n",
        "\n",
        "**Exercise 3.1**: Run the `silver_transformation.py` script from your terminal. Then, read the Silver table and inspect its schema and data.\n",
        "\n",
        "```bash\n",
        "# From your project root directory in the terminal:\n",
        "# PYTHONPATH=$(pwd) python3 scripts/silver_transformation.py\n",
        "```\n",
        "\n",
        "**Exercise 3.2**: Read the Silver table using PySpark and display its content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**Exercise 3.3**: Try to perform an additional transformation. For example, filter the data to only include sales where `quantity` is greater than 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Gold Layer: Aggregated Data for Analytics\n",
        "\n",
        "The Gold layer is optimized for analytics and reporting. Here, we'll aggregate the sales data to get the total revenue per product and city. The `scripts/gold_aggregation.py` script handles this.\n",
        "\n",
        "**Exercise 4.1**: Run the `gold_aggregation.py` script from your terminal. Then, read the Gold table and examine the aggregated results.\n",
        "\n",
        "```bash\n",
        "# From your project root directory in the terminal:\n",
        "# PYTHONPATH=$(pwd) python3 scripts/gold_aggregation.py\n",
        "```\n",
        "\n",
        "**Exercise 4.2**: Read the Gold table using PySpark and display its content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Read the Gold table using Delta format\n",
        "# Hint: Use spark.read.format(\"delta\").load(\"../data/gold/sales_summary\")\n",
        "# Display schema and show all rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**Exercise 4.3**: Use Spark SQL to query the Gold table. For example, find the top 5 products by total revenue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a temporary view and query it using Spark SQL\n",
        "# Hint: First create a temp view, then use spark.sql() to query\n",
        "# Find top 5 products by total revenue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Cleaning Up\n",
        "\n",
        "After you're done practicing, it's good practice to stop your SparkSession.\n",
        "\n",
        "**Exercise 5.1**: Stop the SparkSession.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Stop the SparkSession\n",
        "# Hint: Use spark.stop() and print a confirmation message\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Experiment with different data transformations and aggregations.\n",
        "- Try ingesting data from other formats (e.g., JSON, Parquet) into the Bronze layer.\n",
        "- Explore more advanced PySpark features like UDFs, window functions, and structured streaming.\n",
        "- Apply these concepts to your Databricks projects!\n",
        "\n",
        "Happy PySparking!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "practise_db_book_pyspark_learn (3.11.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
