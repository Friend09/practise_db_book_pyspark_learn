{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2954c0f8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Spark SQL Getting Started - Practice Notebook\n",
    "\n",
    "This notebook covers the fundamentals of Spark SQL based on the [official Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand SparkSession as the entry point to Spark functionality\n",
    "- Create DataFrames from various sources (lists, files)\n",
    "- Perform basic DataFrame operations\n",
    "- Understand the difference between transformations and actions\n",
    "\n",
    "## Sections\n",
    "1. **SparkSession Initialization**\n",
    "2. **Creating DataFrames from Python Data**\n",
    "3. **Creating DataFrames from Files**\n",
    "4. **Basic DataFrame Operations**\n",
    "5. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2792263",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. SparkSession Initialization\n",
    "\n",
    "The **SparkSession** is the entry point to all Spark functionality. It provides a unified interface for working with Spark SQL, DataFrames, and Datasets.\n",
    "\n",
    "### Key Points:\n",
    "- SparkSession replaces the older SparkContext + SQLContext pattern\n",
    "- Use `SparkSession.builder` to create a session\n",
    "- Configure application name and options during creation\n",
    "- Built-in support for Hive features (HiveQL, UDFs, Hive tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b4f78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.0\n",
      "Application Name: Spark SQL getting started\n",
      "Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark SQL getting started\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "print(f\"Application Name: {spark.conf.get('spark.app.name')}\")\n",
    "print(f\"Master: {spark.conf.get('spark.master')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f6a07e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Creating DataFrames from Python Data\n",
    "\n",
    "DataFrames can be created from various Python data structures like lists, tuples, and dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "511e1fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from tuples:\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n",
      "\n",
      "DataFrame from dictionaries:\n",
      "+---+-------------+-------+\n",
      "|age|         city|   name|\n",
      "+---+-------------+-------+\n",
      "| 25|     New York|  Alice|\n",
      "| 30|San Francisco|    Bob|\n",
      "| 35|      Chicago|Charlie|\n",
      "+---+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Create DataFrame from list of tuples\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df_from_tuples = spark.createDataFrame(data, columns)\n",
    "print(\"DataFrame from tuples:\")\n",
    "df_from_tuples.show()\n",
    "\n",
    "# Method 2: Create DataFrame from list of dictionaries\n",
    "data_dict = [\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 30, \"city\": \"San Francisco\"},\n",
    "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"},\n",
    "]\n",
    "\n",
    "df_from_dict = spark.createDataFrame(data_dict)\n",
    "print(\"\\nDataFrame from dictionaries:\")\n",
    "df_from_dict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25181a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of df_from_tuples:\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "\n",
      "Schema of df_from_dict:\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "\n",
      "Number of rows in df_from_dict: 3\n",
      "Number of columns in df_from_dict: 3\n",
      "Column names: ['age', 'city', 'name']\n"
     ]
    }
   ],
   "source": [
    "# Print schema information\n",
    "print(\"Schema of df_from_tuples:\")\n",
    "df_from_tuples.printSchema()\n",
    "\n",
    "print(\"\\nSchema of df_from_dict:\")\n",
    "df_from_dict.printSchema()\n",
    "\n",
    "# Show DataFrame info\n",
    "print(f\"\\nNumber of rows in df_from_dict: {df_from_dict.count()}\")\n",
    "print(f\"Number of columns in df_from_dict: {len(df_from_dict.columns)}\")\n",
    "print(f\"Column names: {df_from_dict.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51905e39",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Creating DataFrames from Files\n",
    "\n",
    "Spark can read data from various file formats including JSON, CSV, and Parquet. Let's create sample files and read them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74297f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data files created successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "\n",
    "sample_data = [\n",
    "    {\"name\": \"Bahadur\", \"age\": None},\n",
    "    {\"name\": \"Jitesh\", \"age\": 25},\n",
    "    {\"name\": \"Ramanand\", \"age\": 32},\n",
    "]\n",
    "\n",
    "with open(\"../data/raw/people.json\", \"w\") as f:\n",
    "    for record in sample_data:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "pd.DataFrame(sample_data).to_csv(\"../data/raw/people.csv\", index=False)\n",
    "\n",
    "print(\"Sample data files created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4fe5bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from JSON:\n",
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|NULL| Bahadur|\n",
      "|  25|  Jitesh|\n",
      "|  32|Ramanand|\n",
      "+----+--------+\n",
      "\n",
      "\n",
      "DataFrame from CSV:\n",
      "+--------+----+\n",
      "|    name| age|\n",
      "+--------+----+\n",
      "| Bahadur|NULL|\n",
      "|  Jitesh|25.0|\n",
      "|Ramanand|32.0|\n",
      "+--------+----+\n",
      "\n",
      "\n",
      "JSON DataFrame schema:\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "\n",
      "CSV DataFrame schema:\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file\n",
    "df_json = spark.read.json(\"../data/raw/people.json\")\n",
    "print(\"DataFrame from JSON:\")\n",
    "df_json.show()\n",
    "\n",
    "# Read CSV file\n",
    "df_csv = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../data/raw/people.csv\")\n",
    ")\n",
    "print(\"\\nDataFrame from CSV:\")\n",
    "df_csv.show()\n",
    "\n",
    "# Compare schemas\n",
    "print(\"\\nJSON DataFrame schema:\")\n",
    "df_json.printSchema()\n",
    "\n",
    "print(\"\\nCSV DataFrame schema:\")\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e5f96",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Basic DataFrame Operations\n",
    "\n",
    "Now let's explore fundamental DataFrame operations including selections, filtering, and transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "337e4698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Select only 'name' column:\n",
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "| Bahadur|\n",
      "|  Jitesh|\n",
      "|Ramanand|\n",
      "+--------+\n",
      "\n",
      "\n",
      "2. Select name and age + 1:\n",
      "+--------+---------+\n",
      "|    name|(age + 1)|\n",
      "+--------+---------+\n",
      "| Bahadur|     NULL|\n",
      "|  Jitesh|       26|\n",
      "|Ramanand|       33|\n",
      "+--------+---------+\n",
      "\n",
      "\n",
      "3. Filter people older than 21:\n",
      "+---+--------+\n",
      "|age|    name|\n",
      "+---+--------+\n",
      "| 25|  Jitesh|\n",
      "| 32|Ramanand|\n",
      "+---+--------+\n",
      "\n",
      "\n",
      "4. Count people by age:\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  32|    1|\n",
      "|  25|    1|\n",
      "|NULL|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the JSON DataFrame for operations\n",
    "df = df_json\n",
    "\n",
    "# 1. Select specific columns\n",
    "print(\"1. Select only 'name' column:\")\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# 2. Select multiple columns with expressions\n",
    "print(\"\\n2. Select name and age + 1:\")\n",
    "df.select(df[\"name\"], df[\"age\"] + 1).show()\n",
    "\n",
    "# 3. Filter rows\n",
    "print(\"\\n3. Filter people older than 21:\")\n",
    "df.filter(df[\"age\"] > 21).show()\n",
    "\n",
    "# 4. Group by and count\n",
    "print(\"\\n4. Count people by age:\")\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8693e8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Add a new column 'is_adult':\n",
      "+----+--------+--------+\n",
      "| age|    name|is_adult|\n",
      "+----+--------+--------+\n",
      "|NULL| Bahadur|    NULL|\n",
      "|  25|  Jitesh|    true|\n",
      "|  32|Ramanand|    true|\n",
      "+----+--------+--------+\n",
      "\n",
      "\n",
      "6. Rename 'name' to 'full_name':\n",
      "+----+---------+\n",
      "| age|full_name|\n",
      "+----+---------+\n",
      "|NULL|  Bahadur|\n",
      "|  25|   Jitesh|\n",
      "|  32| Ramanand|\n",
      "+----+---------+\n",
      "\n",
      "\n",
      "7. Sort by age (ascending):\n",
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|NULL| Bahadur|\n",
      "|  25|  Jitesh|\n",
      "|  32|Ramanand|\n",
      "+----+--------+\n",
      "\n",
      "\n",
      "8. Sort by age (descending):\n",
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|  32|Ramanand|\n",
      "|  25|  Jitesh|\n",
      "|NULL| Bahadur|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Add new columns\n",
    "print(\"5. Add a new column 'is_adult':\")\n",
    "df_with_adult = df.withColumn(\"is_adult\", df[\"age\"] >= 18)\n",
    "df_with_adult.show()\n",
    "\n",
    "# 6. Rename columns\n",
    "print(\"\\n6. Rename 'name' to 'full_name':\")\n",
    "df_renamed = df.withColumnRenamed(\"name\", \"full_name\")\n",
    "df_renamed.show()\n",
    "\n",
    "# 7. Sort data\n",
    "print(\"\\n7. Sort by age (ascending):\")\n",
    "df.orderBy(\"age\").show()\n",
    "\n",
    "print(\"\\n8. Sort by age (descending):\")\n",
    "df.orderBy(df[\"age\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427811e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to practice what you've learned.\n",
    "\n",
    "### Exercise 1: Create Your Own DataFrame\n",
    "Create a DataFrame with information about your favorite books including: title, author, year_published, and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5790abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+--------------+\n",
      "| author|rating|     title|year_published|\n",
      "+-------+------+----------+--------------+\n",
      "|valmiki|     4|   ramayan|          1200|\n",
      "|  badul|     3|mahabharat|          1300|\n",
      "|chimbal|     5|     abhil|          2000|\n",
      "|samarin|     2|     fetul|          2004|\n",
      "| yakuzo|     4|    chidel|          1996|\n",
      "+-------+------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Create your books DataFrame here\n",
    "# TODO: Create a DataFrame with at least 5 books\n",
    "# Include columns: title, author, year_published, rating (1-5)\n",
    "\n",
    "books_data = [\n",
    "    {\"title\": \"ramayan\", \"author\": \"valmiki\", \"year_published\": \"1200\", \"rating\": 4},\n",
    "    {\"title\": \"mahabharat\", \"author\": \"badul\", \"year_published\": \"1300\", \"rating\": 3},\n",
    "    {\"title\": \"abhil\", \"author\": \"chimbal\", \"year_published\": \"2000\", \"rating\": 5},\n",
    "    {\"title\": \"fetul\", \"author\": \"samarin\", \"year_published\": \"2004\", \"rating\": 2},\n",
    "    {\"title\": \"chidel\", \"author\": \"yakuzo\", \"year_published\": \"1996\", \"rating\": 4},\n",
    "]\n",
    "\n",
    "# Create DataFrame and show it\n",
    "# df_books = spark.createDataFrame(books_data, [\"title\", \"author\", \"year_published\", \"rating\"])\n",
    "# df_books.show()\n",
    "\n",
    "df_books = spark.createDataFrame(books_data)\n",
    "df_books.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24826f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Exercise 2: DataFrame Operations\n",
    "Using the books DataFrame you created, perform the following operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "526d3c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     title|rating|\n",
      "+----------+------+\n",
      "|   ramayan|     4|\n",
      "|mahabharat|     3|\n",
      "|     abhil|     5|\n",
      "|     fetul|     2|\n",
      "|    chidel|     4|\n",
      "+----------+------+\n",
      "\n",
      "+-------+------+-------+--------------+\n",
      "| author|rating|  title|year_published|\n",
      "+-------+------+-------+--------------+\n",
      "|valmiki|     4|ramayan|          1200|\n",
      "|chimbal|     5|  abhil|          2000|\n",
      "| yakuzo|     4| chidel|          1996|\n",
      "+-------+------+-------+--------------+\n",
      "\n",
      "+-------+------+----------+--------------+-----------+\n",
      "| author|rating|     title|year_published|age_of_book|\n",
      "+-------+------+----------+--------------+-----------+\n",
      "|valmiki|     4|   ramayan|          1200|        825|\n",
      "|  badul|     3|mahabharat|          1300|        725|\n",
      "|chimbal|     5|     abhil|          2000|         25|\n",
      "|samarin|     2|     fetul|          2004|         21|\n",
      "| yakuzo|     4|    chidel|          1996|         29|\n",
      "+-------+------+----------+--------------+-----------+\n",
      "\n",
      "+-------+------+----------+--------------+\n",
      "| author|rating|     title|year_published|\n",
      "+-------+------+----------+--------------+\n",
      "|chimbal|     5|     abhil|          2000|\n",
      "|valmiki|     4|   ramayan|          1200|\n",
      "| yakuzo|     4|    chidel|          1996|\n",
      "|  badul|     3|mahabharat|          1300|\n",
      "|samarin|     2|     fetul|          2004|\n",
      "+-------+------+----------+--------------+\n",
      "\n",
      "+------+-----+\n",
      "|rating|count|\n",
      "+------+-----+\n",
      "|     4|    2|\n",
      "|     3|    1|\n",
      "|     5|    1|\n",
      "|     2|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: DataFrame Operations\n",
    "# TODO: Complete the following operations\n",
    "\n",
    "# 1. Select only title and rating columns\n",
    "df_books.select(\"title\", \"rating\").show()\n",
    "\n",
    "# 2. Filter books with rating >= 4\n",
    "df_books.filter(df_books[\"rating\"] >= 4).show()\n",
    "\n",
    "# 3. Add a new column 'age_of_book' (current year - year_published)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_books.withColumn(\"age_of_book\", F.lit(2025) - df_books[\"year_published\"]).show()\n",
    "\n",
    "# 4. Sort books by rating in descending order\n",
    "df_books.orderBy(df_books[\"rating\"].desc()).show()\n",
    "\n",
    "# 5. Group by author and count the number of books\n",
    "df_books.groupBy(\"rating\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc278d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Exercise 3: File Operations\n",
    "Create a CSV file with employee data and read it back into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b459aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1fb262f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **SparkSession** - The entry point to Spark functionality\n",
    "2. **Creating DataFrames** - From Python data structures and files\n",
    "3. **Basic Operations** - Select, filter, group, sort, and transform data\n",
    "4. **Schema Inspection** - Understanding DataFrame structure\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the next notebook: `02_dataframe_operations.ipynb` to dive deeper into DataFrame transformations and operations.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html)\n",
    "- [PySpark SQL Module Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practise_db_book_pyspark_learn (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
