{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2954c0f8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Spark SQL Getting Started - Practice Notebook\n",
    "\n",
    "This notebook covers the fundamentals of Spark SQL based on the [official Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand SparkSession as the entry point to Spark functionality\n",
    "- Create DataFrames from various sources (lists, files)\n",
    "- Perform basic DataFrame operations\n",
    "- Understand the difference between transformations and actions\n",
    "\n",
    "## Sections\n",
    "1. **SparkSession Initialization**\n",
    "2. **Creating DataFrames from Python Data**\n",
    "3. **Creating DataFrames from Files**\n",
    "4. **Basic DataFrame Operations**\n",
    "5. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2792263",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. SparkSession Initialization\n",
    "\n",
    "The **SparkSession** is the entry point to all Spark functionality. It provides a unified interface for working with Spark SQL, DataFrames, and Datasets.\n",
    "\n",
    "### Key Points:\n",
    "- SparkSession replaces the older SparkContext + SQLContext pattern\n",
    "- Use `SparkSession.builder` to create a session\n",
    "- Configure application name and options during creation\n",
    "- Built-in support for Hive features (HiveQL, UDFs, Hive tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4f78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70f6a07e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Creating DataFrames from Python Data\n",
    "\n",
    "DataFrames can be created from various Python data structures like lists, tuples, and dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e1fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25181a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51905e39",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Creating DataFrames from Files\n",
    "\n",
    "Spark can read data from various file formats including JSON, CSV, and Parquet. Let's create sample files and read them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74297f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe5bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "127e5f96",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Basic DataFrame Operations\n",
    "\n",
    "Now let's explore fundamental DataFrame operations including selections, filtering, and transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e4698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693e8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4427811e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to practice what you've learned.\n",
    "\n",
    "### Exercise 1: Create Your Own DataFrame\n",
    "Create a DataFrame with information about your favorite books including: title, author, year_published, and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790abec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c24826f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Exercise 2: DataFrame Operations\n",
    "Using the books DataFrame you created, perform the following operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d3c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcc278d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Exercise 3: File Operations\n",
    "Create a CSV file with employee data and read it back into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b459aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1fb262f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **SparkSession** - The entry point to Spark functionality\n",
    "2. **Creating DataFrames** - From Python data structures and files\n",
    "3. **Basic Operations** - Select, filter, group, sort, and transform data\n",
    "4. **Schema Inspection** - Understanding DataFrame structure\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the next notebook: `02_dataframe_operations.ipynb` to dive deeper into DataFrame transformations and operations.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html)\n",
    "- [PySpark SQL Module Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
