{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3c938c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Running SQL Queries - Practice Notebook\n",
    "\n",
    "This notebook covers **Running SQL Queries Programmatically** and **Global Temporary Views** from the [Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html).\n",
    "\n",
    "## Learning Objectives\n",
    "- Register DataFrames as temporary views\n",
    "- Execute SQL queries using spark.sql()\n",
    "- Understand the difference between temporary and global temporary views\n",
    "- Compare DataFrame API vs SQL syntax\n",
    "- Practice complex SQL queries\n",
    "\n",
    "## Sections\n",
    "1. **Setup and Data Preparation**\n",
    "2. **Creating Temporary Views**\n",
    "3. **Running SQL Queries**\n",
    "4. **Global Temporary Views**\n",
    "5. **DataFrame API vs SQL Comparison**\n",
    "6. **Complex SQL Queries**\n",
    "7. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f968a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/10 06:06:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+----------+\n",
      "| id|   name| department|salary| hire_date|\n",
      "+---+-------+-----------+------+----------+\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28|\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|\n",
      "|  6|  Frank|Engineering| 82000|2020-09-12|\n",
      "+---+-------+-----------+------+----------+\n",
      "\n",
      "Departments DataFrame:\n",
      "+-----------+--------+-------------+\n",
      "|  dept_name|division|      manager|\n",
      "+-----------+--------+-------------+\n",
      "|Engineering|    Tech|Alice Johnson|\n",
      "|      Sales|Business|    Bob Smith|\n",
      "|  Marketing|Business|Charlie Brown|\n",
      "|         HR| Support| Diana Prince|\n",
      "+-----------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"SQL Queries Practice\").getOrCreate()\n",
    "\n",
    "# Create sample datasets\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\"),\n",
    "    (2, \"Bob\", \"Sales\", 65000, \"2019-03-20\"),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000, \"2018-06-10\"),\n",
    "    (4, \"Diana\", \"Marketing\", 70000, \"2021-02-28\"),\n",
    "    (5, \"Eve\", \"Sales\", 68000, \"2017-11-05\"),\n",
    "    (6, \"Frank\", \"Engineering\", 82000, \"2020-09-12\"),\n",
    "]\n",
    "\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"Tech\", \"Alice Johnson\"),\n",
    "    (\"Sales\", \"Business\", \"Bob Smith\"),\n",
    "    (\"Marketing\", \"Business\", \"Charlie Brown\"),\n",
    "    (\"HR\", \"Support\", \"Diana Prince\"),\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data, [\"id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "departments_df = spark.createDataFrame(\n",
    "    departments_data, [\"dept_name\", \"division\", \"manager\"]\n",
    ")\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"Departments DataFrame:\")\n",
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68033a7b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Creating Temporary Views\n",
    "\n",
    "To run SQL queries on DataFrames, we first need to register them as temporary views.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fd82e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary views created successfully!\n",
      "\n",
      "Current temporary views:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='departments', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register DataFrames as temporary views\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "print(\"Temporary views created successfully!\")\n",
    "\n",
    "# List all temporary views\n",
    "print(\"\\nCurrent temporary views:\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27b05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------+\n",
      "|  dept_name|division|      manager|\n",
      "+-----------+--------+-------------+\n",
      "|Engineering|    Tech|Alice Johnson|\n",
      "|      Sales|Business|    Bob Smith|\n",
      "|  Marketing|Business|Charlie Brown|\n",
      "|         HR| Support| Diana Prince|\n",
      "+-----------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24acdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+----------+\n",
      "| id|   name| department|salary| hire_date|\n",
      "+---+-------+-----------+------+----------+\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28|\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|\n",
      "|  6|  Frank|Engineering| 82000|2020-09-12|\n",
      "+---+-------+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27526d7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Running Basic SQL Queries\n",
    "\n",
    "Now we can run SQL queries using the `spark.sql()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4854b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Select all employees:\n",
      "+---+-------+-----------+------+----------+\n",
      "| id|   name| department|salary| hire_date|\n",
      "+---+-------+-----------+------+----------+\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|\n",
      "|  2|    Bob|      Sales| 65000|2019-03-20|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|\n",
      "|  4|  Diana|  Marketing| 70000|2021-02-28|\n",
      "|  5|    Eve|      Sales| 68000|2017-11-05|\n",
      "|  6|  Frank|Engineering| 82000|2020-09-12|\n",
      "+---+-------+-----------+------+----------+\n",
      "\n",
      "\n",
      "2. Select specific columns:\n",
      "+-------+-----------+------+\n",
      "|   name| department|salary|\n",
      "+-------+-----------+------+\n",
      "|  Alice|Engineering| 75000|\n",
      "|    Bob|      Sales| 65000|\n",
      "|Charlie|Engineering| 80000|\n",
      "|  Diana|  Marketing| 70000|\n",
      "|    Eve|      Sales| 68000|\n",
      "|  Frank|Engineering| 82000|\n",
      "+-------+-----------+------+\n",
      "\n",
      "\n",
      "3. Filter with WHERE clause:\n",
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|  Alice| 75000|\n",
      "|Charlie| 80000|\n",
      "|  Frank| 82000|\n",
      "+-------+------+\n",
      "\n",
      "\n",
      "4. Order by salary:\n",
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|  Frank| 82000|\n",
      "|Charlie| 80000|\n",
      "|  Alice| 75000|\n",
      "|  Diana| 70000|\n",
      "|    Eve| 68000|\n",
      "|    Bob| 65000|\n",
      "+-------+------+\n",
      "\n",
      "\n",
      "5. Count employees by department:\n",
      "+-----------+--------------+\n",
      "| department|employee_count|\n",
      "+-----------+--------------+\n",
      "|Engineering|             3|\n",
      "|      Sales|             2|\n",
      "|  Marketing|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic SELECT queries\n",
    "print(\"1. Select all employees:\")\n",
    "result1 = spark.sql(\"SELECT * FROM employees\")\n",
    "result1.show()\n",
    "\n",
    "print(\"\\n2. Select specific columns:\")\n",
    "result2 = spark.sql(\"SELECT name, department, salary FROM employees\")\n",
    "result2.show()\n",
    "\n",
    "print(\"\\n3. Filter with WHERE clause:\")\n",
    "result3 = spark.sql(\"SELECT name, salary FROM employees WHERE salary > 70000\")\n",
    "result3.show()\n",
    "\n",
    "print(\"\\n4. Order by salary:\")\n",
    "result4 = spark.sql(\"SELECT name, salary FROM employees ORDER BY salary DESC\")\n",
    "result4.show()\n",
    "\n",
    "print(\"\\n5. Count employees by department:\")\n",
    "result5 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT department, COUNT(*) as employee_count\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\"\n",
    ")\n",
    "result5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250068cd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Global Temporary Views\n",
    "\n",
    "Global temporary views are shared across multiple SparkSessions and are kept alive until the Spark application terminates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c54308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global temporary view created!\n",
      "\n",
      "Access global temporary view:\n",
      "+---+-------+-----------+------+----------+\n",
      "| id|   name| department|salary| hire_date|\n",
      "+---+-------+-----------+------+----------+\n",
      "|  1|  Alice|Engineering| 75000|2020-01-15|\n",
      "|  3|Charlie|Engineering| 80000|2018-06-10|\n",
      "|  6|  Frank|Engineering| 82000|2020-09-12|\n",
      "+---+-------+-----------+------+----------+\n",
      "\n",
      "\n",
      "Difference between temporary and global temporary views:\n",
      "- Temporary views: Session-scoped, accessed directly by name\n",
      "- Global temporary views: Application-scoped, accessed via global_temp.view_name\n"
     ]
    }
   ],
   "source": [
    "# Create or replace global temporary view\n",
    "employees_df.createOrReplaceGlobalTempView(\"global_employees\")\n",
    "\n",
    "print(\"Global temporary view created!\")\n",
    "\n",
    "# Access global temporary view (note the global_temp prefix)\n",
    "print(\"\\nAccess global temporary view:\")\n",
    "result_global = spark.sql(\n",
    "    \"SELECT * FROM global_temp.global_employees WHERE department = 'Engineering'\"\n",
    ")\n",
    "result_global.show()\n",
    "\n",
    "# You can also access it from a different SparkSession\n",
    "# new_spark = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
    "# result_from_new_session = new_spark.sql(\"SELECT COUNT(*) FROM global_temp.global_employees\")\n",
    "# result_from_new_session.show()\n",
    "\n",
    "print(\"\\nDifference between temporary and global temporary views:\")\n",
    "print(\"- Temporary views: Session-scoped, accessed directly by name\")\n",
    "print(\n",
    "    \"- Global temporary views: Application-scoped, accessed via global_temp.view_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877494b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. DataFrame API vs SQL Comparison\n",
    "\n",
    "Let's compare the same operations using DataFrame API and SQL syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91638705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXAMPLE 1: Filter and Select ===\n",
      "DataFrame API:\n",
      "+-------+-----------+------+\n",
      "|   name| department|salary|\n",
      "+-------+-----------+------+\n",
      "|  Alice|Engineering| 75000|\n",
      "|Charlie|Engineering| 80000|\n",
      "|  Frank|Engineering| 82000|\n",
      "+-------+-----------+------+\n",
      "\n",
      "SQL:\n",
      "+-------+-----------+------+\n",
      "|   name| department|salary|\n",
      "+-------+-----------+------+\n",
      "|  Alice|Engineering| 75000|\n",
      "|Charlie|Engineering| 80000|\n",
      "|  Frank|Engineering| 82000|\n",
      "+-------+-----------+------+\n",
      "\n",
      "\n",
      "=== EXAMPLE 2: Group By and Aggregate ===\n",
      "DataFrame API:\n",
      "+-----------+-----+----------+----------+\n",
      "| department|count|avg_salary|max_salary|\n",
      "+-----------+-----+----------+----------+\n",
      "|Engineering|    3|   79000.0|     82000|\n",
      "|      Sales|    2|   66500.0|     68000|\n",
      "|  Marketing|    1|   70000.0|     70000|\n",
      "+-----------+-----+----------+----------+\n",
      "\n",
      "SQL:\n",
      "+-----------+-----+----------+----------+\n",
      "| department|count|avg_salary|max_salary|\n",
      "+-----------+-----+----------+----------+\n",
      "|Engineering|    3|   79000.0|     82000|\n",
      "|      Sales|    2|   66500.0|     68000|\n",
      "|  Marketing|    1|   70000.0|     70000|\n",
      "+-----------+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Filter and select\n",
    "print(\"=== EXAMPLE 1: Filter and Select ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example 2: Group by and aggregate\n",
    "print(\"\\n=== EXAMPLE 2: Group By and Aggregate ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ce9ad",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Complex SQL Queries\n",
    "\n",
    "Practice more advanced SQL operations including joins, subqueries, and window functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ea216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139b842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "211080cd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Practice Exercises\n",
    "\n",
    "Complete these SQL exercises to test your understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc59c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practise_db_book_pyspark_learn (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
