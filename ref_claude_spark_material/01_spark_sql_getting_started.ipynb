{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Spark SQL Getting Started - Practice Notebook\n",
    "\n",
    "This notebook covers the fundamentals of Spark SQL based on the [official Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand SparkSession as the entry point to Spark functionality\n",
    "- Create DataFrames from various sources (lists, files)\n",
    "- Perform basic DataFrame operations\n",
    "- Understand the difference between transformations and actions\n",
    "\n",
    "## Sections\n",
    "1. **SparkSession Initialization**\n",
    "2. **Creating DataFrames from Python Data**\n",
    "3. **Creating DataFrames from Files**\n",
    "4. **Basic DataFrame Operations**\n",
    "5. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2792263",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. SparkSession Initialization\n",
    "\n",
    "The **SparkSession** is the entry point to all Spark functionality. It provides a unified interface for working with Spark SQL, DataFrames, and Datasets.\n",
    "\n",
    "### Key Points:\n",
    "- SparkSession replaces the older SparkContext + SQLContext pattern\n",
    "- Use `SparkSession.builder` to create a session\n",
    "- Configure application name and options during creation\n",
    "- Built-in support for Hive features (HiveQL, UDFs, Hive tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession - the entry point to all Spark functionality\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL Getting Started\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark version\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# Display SparkSession info\n",
    "print(f\"Application Name: {spark.conf.get('spark.app.name')}\")\n",
    "print(f\"Master: {spark.conf.get('spark.master')}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Creating DataFrames from Python Data\n",
    "\n",
    "DataFrames can be created from various Python data structures like lists, tuples, and dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Create DataFrame from list of tuples\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df_from_tuples = spark.createDataFrame(data, columns)\n",
    "print(\"DataFrame from tuples:\")\n",
    "df_from_tuples.show()\n",
    "\n",
    "# Method 2: Create DataFrame from list of dictionaries\n",
    "data_dict = [\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 30, \"city\": \"San Francisco\"},\n",
    "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"}\n",
    "]\n",
    "\n",
    "df_from_dict = spark.createDataFrame(data_dict)\n",
    "print(\"\\nDataFrame from dictionaries:\")\n",
    "df_from_dict.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25181a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema information\n",
    "print(\"Schema of df_from_tuples:\")\n",
    "df_from_tuples.printSchema()\n",
    "\n",
    "print(\"\\nSchema of df_from_dict:\")\n",
    "df_from_dict.printSchema()\n",
    "\n",
    "# Show DataFrame info\n",
    "print(f\"\\nNumber of rows in df_from_dict: {df_from_dict.count()}\")\n",
    "print(f\"Number of columns in df_from_dict: {len(df_from_dict.columns)}\")\n",
    "print(f\"Column names: {df_from_dict.columns}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Creating DataFrames from Files\n",
    "\n",
    "Spark can read data from various file formats including JSON, CSV, and Parquet. Let's create sample files and read them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74297f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create sample data files for practice\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "# Sample data\n",
    "sample_data = [\n",
    "    {\"name\": \"Michael\", \"age\": None},\n",
    "    {\"name\": \"Andy\", \"age\": 30},\n",
    "    {\"name\": \"Justin\", \"age\": 19}\n",
    "]\n",
    "\n",
    "# Create JSON file\n",
    "with open(\"../data/people.json\", \"w\") as f:\n",
    "    for record in sample_data:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# Create CSV file\n",
    "pd.DataFrame(sample_data).to_csv(\"../data/people.csv\", index=False)\n",
    "\n",
    "print(\"Sample data files created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "df_json = spark.read.json(\"../data/people.json\")\n",
    "print(\"DataFrame from JSON:\")\n",
    "df_json.show()\n",
    "\n",
    "# Read CSV file\n",
    "df_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"../data/people.csv\")\n",
    "print(\"\\nDataFrame from CSV:\")\n",
    "df_csv.show()\n",
    "\n",
    "# Compare schemas\n",
    "print(\"\\nJSON DataFrame schema:\")\n",
    "df_json.printSchema()\n",
    "\n",
    "print(\"\\nCSV DataFrame schema:\")\n",
    "df_csv.printSchema()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Basic DataFrame Operations\n",
    "\n",
    "Now let's explore fundamental DataFrame operations including selections, filtering, and transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the JSON DataFrame for operations\n",
    "df = df_json\n",
    "\n",
    "# 1. Select specific columns\n",
    "print(\"1. Select only 'name' column:\")\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# 2. Select multiple columns with expressions\n",
    "print(\"\\n2. Select name and age + 1:\")\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "\n",
    "# 3. Filter rows\n",
    "print(\"\\n3. Filter people older than 21:\")\n",
    "df.filter(df['age'] > 21).show()\n",
    "\n",
    "# 4. Group by and count\n",
    "print(\"\\n4. Count people by age:\")\n",
    "df.groupBy(\"age\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Add new columns\n",
    "print(\"5. Add a new column 'is_adult':\")\n",
    "df_with_adult = df.withColumn(\"is_adult\", df['age'] >= 18)\n",
    "df_with_adult.show()\n",
    "\n",
    "# 6. Rename columns\n",
    "print(\"\\n6. Rename 'name' to 'full_name':\")\n",
    "df_renamed = df.withColumnRenamed(\"name\", \"full_name\")\n",
    "df_renamed.show()\n",
    "\n",
    "# 7. Sort data\n",
    "print(\"\\n7. Sort by age (ascending):\")\n",
    "df.orderBy(\"age\").show()\n",
    "\n",
    "print(\"\\n8. Sort by age (descending):\")\n",
    "df.orderBy(df['age'].desc()).show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to practice what you've learned.\n",
    "\n",
    "### Exercise 1: Create Your Own DataFrame\n",
    "Create a DataFrame with information about your favorite books including: title, author, year_published, and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create your books DataFrame here\n",
    "# TODO: Create a DataFrame with at least 5 books\n",
    "# Include columns: title, author, year_published, rating (1-5)\n",
    "\n",
    "books_data = [\n",
    "    # Add your data here\n",
    "]\n",
    "\n",
    "# Create DataFrame and show it\n",
    "# df_books = spark.createDataFrame(books_data, [\"title\", \"author\", \"year_published\", \"rating\"])\n",
    "# df_books.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Exercise 2: DataFrame Operations\n",
    "Using the books DataFrame you created, perform the following operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: DataFrame Operations\n",
    "# TODO: Complete the following operations\n",
    "\n",
    "# 1. Select only title and rating columns\n",
    "# your_code_here\n",
    "\n",
    "# 2. Filter books with rating >= 4\n",
    "# your_code_here\n",
    "\n",
    "# 3. Add a new column 'age_of_book' (current year - year_published)\n",
    "# your_code_here\n",
    "\n",
    "# 4. Sort books by rating in descending order\n",
    "# your_code_here\n",
    "\n",
    "# 5. Group by author and count the number of books\n",
    "# your_code_here\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Exercise 3: File Operations\n",
    "Create a CSV file with employee data and read it back into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b459aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: File Operations\n",
    "# TODO: Complete the following\n",
    "\n",
    "# 1. Create employee data as a list of dictionaries\n",
    "# Include: employee_id, name, department, salary\n",
    "employees = [\n",
    "    # Add your employee data here\n",
    "]\n",
    "\n",
    "# 2. Convert to pandas DataFrame and save as CSV\n",
    "# your_code_here\n",
    "\n",
    "# 3. Read the CSV file back using Spark\n",
    "# your_code_here\n",
    "\n",
    "# 4. Display the DataFrame and its schema\n",
    "# your_code_here\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **SparkSession** - The entry point to Spark functionality\n",
    "2. **Creating DataFrames** - From Python data structures and files\n",
    "3. **Basic Operations** - Select, filter, group, sort, and transform data\n",
    "4. **Schema Inspection** - Understanding DataFrame structure\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the next notebook: `02_dataframe_operations.ipynb` to dive deeper into DataFrame transformations and operations.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html)\n",
    "- [PySpark SQL Module Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
