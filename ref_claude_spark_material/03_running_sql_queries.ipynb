{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Running SQL Queries - Practice Notebook\n",
    "\n",
    "This notebook covers **Running SQL Queries Programmatically** and **Global Temporary Views** from the [Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html).\n",
    "\n",
    "## Learning Objectives\n",
    "- Register DataFrames as temporary views\n",
    "- Execute SQL queries using spark.sql()\n",
    "- Understand the difference between temporary and global temporary views\n",
    "- Compare DataFrame API vs SQL syntax\n",
    "- Practice complex SQL queries\n",
    "\n",
    "## Sections\n",
    "1. **Setup and Data Preparation**\n",
    "2. **Creating Temporary Views**\n",
    "3. **Running SQL Queries**\n",
    "4. **Global Temporary Views**\n",
    "5. **DataFrame API vs SQL Comparison**\n",
    "6. **Complex SQL Queries**\n",
    "7. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f968a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"SQL Queries Practice\").getOrCreate()\n",
    "\n",
    "# Create sample datasets\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\"),\n",
    "    (2, \"Bob\", \"Sales\", 65000, \"2019-03-20\"),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000, \"2018-06-10\"),\n",
    "    (4, \"Diana\", \"Marketing\", 70000, \"2021-02-28\"),\n",
    "    (5, \"Eve\", \"Sales\", 68000, \"2017-11-05\"),\n",
    "    (6, \"Frank\", \"Engineering\", 82000, \"2020-09-12\")\n",
    "]\n",
    "\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"Tech\", \"Alice Johnson\"),\n",
    "    (\"Sales\", \"Business\", \"Bob Smith\"),\n",
    "    (\"Marketing\", \"Business\", \"Charlie Brown\"),\n",
    "    (\"HR\", \"Support\", \"Diana Prince\")\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, [\"id\", \"name\", \"department\", \"salary\", \"hire_date\"])\n",
    "departments_df = spark.createDataFrame(departments_data, [\"dept_name\", \"division\", \"manager\"])\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"Departments DataFrame:\")\n",
    "departments_df.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Creating Temporary Views\n",
    "\n",
    "To run SQL queries on DataFrames, we first need to register them as temporary views.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd82e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrames as temporary views\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "print(\"Temporary views created successfully!\")\n",
    "\n",
    "# List all temporary views\n",
    "print(\"\\nCurrent temporary views:\")\n",
    "spark.catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Running Basic SQL Queries\n",
    "\n",
    "Now we can run SQL queries using the `spark.sql()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SELECT queries\n",
    "print(\"1. Select all employees:\")\n",
    "result1 = spark.sql(\"SELECT * FROM employees\")\n",
    "result1.show()\n",
    "\n",
    "print(\"\\n2. Select specific columns:\")\n",
    "result2 = spark.sql(\"SELECT name, department, salary FROM employees\")\n",
    "result2.show()\n",
    "\n",
    "print(\"\\n3. Filter with WHERE clause:\")\n",
    "result3 = spark.sql(\"SELECT name, salary FROM employees WHERE salary > 70000\")\n",
    "result3.show()\n",
    "\n",
    "print(\"\\n4. Order by salary:\")\n",
    "result4 = spark.sql(\"SELECT name, salary FROM employees ORDER BY salary DESC\")\n",
    "result4.show()\n",
    "\n",
    "print(\"\\n5. Count employees by department:\")\n",
    "result5 = spark.sql(\"\"\"\n",
    "    SELECT department, COUNT(*) as employee_count\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "result5.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Global Temporary Views\n",
    "\n",
    "Global temporary views are shared across multiple SparkSessions and are kept alive until the Spark application terminates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c54308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global temporary view\n",
    "employees_df.createGlobalTempView(\"global_employees\")\n",
    "\n",
    "print(\"Global temporary view created!\")\n",
    "\n",
    "# Access global temporary view (note the global_temp prefix)\n",
    "print(\"\\nAccess global temporary view:\")\n",
    "result_global = spark.sql(\"SELECT * FROM global_temp.global_employees WHERE department = 'Engineering'\")\n",
    "result_global.show()\n",
    "\n",
    "# You can also access it from a different SparkSession\n",
    "# new_spark = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
    "# result_from_new_session = new_spark.sql(\"SELECT COUNT(*) FROM global_temp.global_employees\")\n",
    "# result_from_new_session.show()\n",
    "\n",
    "print(\"\\nDifference between temporary and global temporary views:\")\n",
    "print(\"- Temporary views: Session-scoped, accessed directly by name\")\n",
    "print(\"- Global temporary views: Application-scoped, accessed via global_temp.view_name\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. DataFrame API vs SQL Comparison\n",
    "\n",
    "Let's compare the same operations using DataFrame API and SQL syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91638705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Filter and select\n",
    "print(\"=== EXAMPLE 1: Filter and Select ===\")\n",
    "\n",
    "# DataFrame API\n",
    "print(\"DataFrame API:\")\n",
    "df_api_result = employees_df.filter(employees_df.salary > 70000).select(\"name\", \"department\", \"salary\")\n",
    "df_api_result.show()\n",
    "\n",
    "# SQL\n",
    "print(\"SQL:\")\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 70000\n",
    "\"\"\")\n",
    "sql_result.show()\n",
    "\n",
    "# Example 2: Group by and aggregate\n",
    "print(\"\\n=== EXAMPLE 2: Group By and Aggregate ===\")\n",
    "\n",
    "# DataFrame API\n",
    "print(\"DataFrame API:\")\n",
    "df_api_agg = employees_df.groupBy(\"department\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\")\n",
    ")\n",
    "df_api_agg.show()\n",
    "\n",
    "# SQL\n",
    "print(\"SQL:\")\n",
    "sql_agg = spark.sql(\"\"\"\n",
    "    SELECT department,\n",
    "           COUNT(*) as count,\n",
    "           AVG(salary) as avg_salary,\n",
    "           MAX(salary) as max_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "sql_agg.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Complex SQL Queries\n",
    "\n",
    "Practice more advanced SQL operations including joins, subqueries, and window functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ea216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN operations\n",
    "print(\"=== JOIN OPERATIONS ===\")\n",
    "\n",
    "print(\"1. Inner join employees with departments:\")\n",
    "join_result = spark.sql(\"\"\"\n",
    "    SELECT e.name, e.salary, d.division, d.manager\n",
    "    FROM employees e\n",
    "    INNER JOIN departments d ON e.department = d.dept_name\n",
    "\"\"\")\n",
    "join_result.show()\n",
    "\n",
    "print(\"\\n2. Left join to include all employees:\")\n",
    "left_join_result = spark.sql(\"\"\"\n",
    "    SELECT e.name, e.department, e.salary, d.division\n",
    "    FROM employees e\n",
    "    LEFT JOIN departments d ON e.department = d.dept_name\n",
    "\"\"\")\n",
    "left_join_result.show()\n",
    "\n",
    "# Subqueries\n",
    "print(\"\\n=== SUBQUERIES ===\")\n",
    "\n",
    "print(\"3. Employees with above-average salary:\")\n",
    "subquery_result = spark.sql(\"\"\"\n",
    "    SELECT name, salary\n",
    "    FROM employees\n",
    "    WHERE salary > (SELECT AVG(salary) FROM employees)\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "subquery_result.show()\n",
    "\n",
    "print(\"\\n4. Department with highest average salary:\")\n",
    "dept_avg_result = spark.sql(\"\"\"\n",
    "    SELECT department, avg_salary\n",
    "    FROM (\n",
    "        SELECT department, AVG(salary) as avg_salary\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    ) dept_avg\n",
    "    ORDER BY avg_salary DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "dept_avg_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions\n",
    "print(\"=== WINDOW FUNCTIONS ===\")\n",
    "\n",
    "print(\"5. Rank employees by salary within each department:\")\n",
    "window_result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary,\n",
    "           RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank_in_dept,\n",
    "           ROW_NUMBER() OVER (ORDER BY salary DESC) as overall_rank\n",
    "    FROM employees\n",
    "\"\"\")\n",
    "window_result.show()\n",
    "\n",
    "print(\"\\n6. Running total of salaries:\")\n",
    "running_total_result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary,\n",
    "           SUM(salary) OVER (ORDER BY salary ROWS UNBOUNDED PRECEDING) as running_total\n",
    "    FROM employees\n",
    "    ORDER BY salary\n",
    "\"\"\")\n",
    "running_total_result.show()\n",
    "\n",
    "# Common Table Expressions (CTEs)\n",
    "print(\"\\n=== COMMON TABLE EXPRESSIONS ===\")\n",
    "\n",
    "print(\"7. Using CTE to find top performers:\")\n",
    "cte_result = spark.sql(\"\"\"\n",
    "    WITH dept_stats AS (\n",
    "        SELECT department, AVG(salary) as avg_salary\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    ),\n",
    "    top_performers AS (\n",
    "        SELECT e.name, e.department, e.salary\n",
    "        FROM employees e\n",
    "        JOIN dept_stats d ON e.department = d.department\n",
    "        WHERE e.salary > d.avg_salary\n",
    "    )\n",
    "    SELECT * FROM top_performers\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "cte_result.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Practice Exercises\n",
    "\n",
    "Complete these SQL exercises to test your understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional data for exercises\n",
    "products_data = [\n",
    "    (1, \"Laptop\", \"Electronics\", 1200, 50),\n",
    "    (2, \"Mouse\", \"Electronics\", 25, 200),\n",
    "    (3, \"Keyboard\", \"Electronics\", 75, 150),\n",
    "    (4, \"Chair\", \"Furniture\", 300, 30),\n",
    "    (5, \"Desk\", \"Furniture\", 500, 20),\n",
    "    (6, \"Book\", \"Education\", 15, 1000)\n",
    "]\n",
    "\n",
    "orders_data = [\n",
    "    (101, 1, 2, \"2024-01-15\"),\n",
    "    (102, 2, 5, \"2024-01-16\"),\n",
    "    (103, 1, 1, \"2024-01-17\"),\n",
    "    (104, 3, 3, \"2024-01-18\"),\n",
    "    (105, 4, 1, \"2024-01-19\"),\n",
    "    (106, 5, 2, \"2024-01-20\")\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"category\", \"price\", \"stock\"])\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"product_id\", \"quantity\", \"order_date\"])\n",
    "\n",
    "# Register as temp views\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "print(\"Products table:\")\n",
    "products_df.show()\n",
    "print(\"Orders table:\")\n",
    "orders_df.show()\n",
    "\n",
    "# TODO: Complete the following exercises using SQL\n",
    "\n",
    "print(\"\\n=== EXERCISE 1 ===\")\n",
    "print(\"Find all products with stock less than 100\")\n",
    "# Your SQL query here\n",
    "# result1 = spark.sql(\"YOUR QUERY HERE\")\n",
    "# result1.show()\n",
    "\n",
    "print(\"\\n=== EXERCISE 2 ===\")\n",
    "print(\"Calculate total revenue by category\")\n",
    "# Your SQL query here (hint: join products and orders, then group by category)\n",
    "# result2 = spark.sql(\"YOUR QUERY HERE\")\n",
    "# result2.show()\n",
    "\n",
    "print(\"\\n=== EXERCISE 3 ===\")\n",
    "print(\"Find the most expensive product in each category\")\n",
    "# Your SQL query here (hint: use window functions)\n",
    "# result3 = spark.sql(\"YOUR QUERY HERE\")\n",
    "# result3.show()\n",
    "\n",
    "print(\"\\n=== EXERCISE 4 ===\")\n",
    "print(\"List products that have never been ordered\")\n",
    "# Your SQL query here (hint: use LEFT JOIN or NOT EXISTS)\n",
    "# result4 = spark.sql(\"YOUR QUERY HERE\")\n",
    "# result4.show()\n",
    "\n",
    "print(\"\\n=== EXERCISE 5 ===\")\n",
    "print(\"Create a report showing order details with product information\")\n",
    "# Your SQL query here\n",
    "# result5 = spark.sql(\"YOUR QUERY HERE\")\n",
    "# result5.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
