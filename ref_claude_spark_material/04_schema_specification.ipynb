{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Schema Specification and RDD Interoperability - Practice Notebook\n",
    "\n",
    "This notebook covers **Interoperating with RDDs** and **Programmatically Specifying Schema** from the [Spark SQL Getting Started Guide](https://spark.apache.org/docs/latest/sql-getting-started.html).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand RDD to DataFrame conversion\n",
    "- Learn schema inference vs explicit schema definition\n",
    "- Practice creating DataFrames with custom schemas\n",
    "- Work with StructType and StructField\n",
    "- Handle complex data types\n",
    "\n",
    "## Sections\n",
    "1. **Setup and Basic RDD Operations**\n",
    "2. **Schema Inference from RDDs**\n",
    "3. **Programmatically Specifying Schema**\n",
    "4. **Working with Complex Data Types**\n",
    "5. **Schema Evolution and Validation**\n",
    "6. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Schema Specification\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"SparkSession and SparkContext created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Basic RDD Operations and DataFrame Conversion\n",
    "\n",
    "First, let's understand how to work with RDDs and convert them to DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from text data (simulating reading from a file)\n",
    "text_data = [\n",
    "    \"Alice,25,Engineer\",\n",
    "    \"Bob,30,Manager\",\n",
    "    \"Charlie,35,Engineer\",\n",
    "    \"Diana,28,Analyst\"\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "lines_rdd = sc.parallelize(text_data)\n",
    "print(\"Original RDD:\")\n",
    "print(lines_rdd.collect())\n",
    "\n",
    "# Parse the RDD to create structured data\n",
    "def parse_line(line):\n",
    "    parts = line.split(\",\")\n",
    "    return (parts[0], int(parts[1]), parts[2])\n",
    "\n",
    "parsed_rdd = lines_rdd.map(parse_line)\n",
    "print(\"\\nParsed RDD:\")\n",
    "print(parsed_rdd.collect())\n",
    "\n",
    "# Convert RDD to DataFrame with automatic schema inference\n",
    "df_inferred = spark.createDataFrame(parsed_rdd, [\"name\", \"age\", \"job\"])\n",
    "print(\"\\nDataFrame with inferred schema:\")\n",
    "df_inferred.show()\n",
    "df_inferred.printSchema()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Programmatically Specifying Schema\n",
    "\n",
    "When schema cannot be inferred or needs to be controlled precisely, we can define it programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema programmatically\n",
    "custom_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"job\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Custom schema definition:\")\n",
    "print(custom_schema)\n",
    "\n",
    "# Create DataFrame with custom schema\n",
    "df_custom_schema = spark.createDataFrame(parsed_rdd, custom_schema)\n",
    "print(\"\\nDataFrame with custom schema:\")\n",
    "df_custom_schema.show()\n",
    "df_custom_schema.printSchema()\n",
    "\n",
    "# More complex schema example\n",
    "complex_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), False),  # Not nullable\n",
    "    StructField(\"personal_info\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"email\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"job_details\", StructType([\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"salary\", DoubleType(), True),\n",
    "        StructField(\"start_date\", DateType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "print(\"\\nComplex nested schema:\")\n",
    "print(complex_schema.simpleString())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Working with Different Data Types\n",
    "\n",
    "Explore various Spark SQL data types and how to use them in schema definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate various data types\n",
    "from datetime import date, datetime\n",
    "from decimal import Decimal\n",
    "\n",
    "# Create sample data with different types\n",
    "sample_data = [\n",
    "    Row(\n",
    "        id=1,\n",
    "        name=\"Alice\",\n",
    "        salary=75000.50,\n",
    "        is_active=True,\n",
    "        hire_date=date(2020, 1, 15),\n",
    "        last_login=datetime(2024, 1, 10, 14, 30, 0),\n",
    "        bonus=Decimal(\"5000.25\"),\n",
    "        skills=[\"Python\", \"SQL\", \"Spark\"],\n",
    "        metadata={\"department\": \"Engineering\", \"level\": \"Senior\"}\n",
    "    ),\n",
    "    Row(\n",
    "        id=2,\n",
    "        name=\"Bob\",\n",
    "        salary=85000.75,\n",
    "        is_active=False,\n",
    "        hire_date=date(2019, 3, 20),\n",
    "        last_login=datetime(2024, 1, 9, 9, 15, 0),\n",
    "        bonus=Decimal(\"7500.00\"),\n",
    "        skills=[\"Java\", \"Scala\", \"Kafka\"],\n",
    "        metadata={\"department\": \"Engineering\", \"level\": \"Lead\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create DataFrame from Row objects (schema inferred)\n",
    "df_various_types = spark.createDataFrame(sample_data)\n",
    "print(\"DataFrame with various data types:\")\n",
    "df_various_types.show(truncate=False)\n",
    "df_various_types.printSchema()\n",
    "\n",
    "# Define explicit schema for the same data\n",
    "explicit_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True),\n",
    "    StructField(\"last_login\", TimestampType(), True),\n",
    "    StructField(\"bonus\", DecimalType(10, 2), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True),\n",
    "    StructField(\"metadata\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "print(\"\\nExplicit schema for various data types:\")\n",
    "print(explicit_schema.simpleString())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Schema Validation and Error Handling\n",
    "\n",
    "Learn how to handle schema mismatches and validation errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema validation examples\n",
    "print(\"=== SCHEMA VALIDATION ===\")\n",
    "\n",
    "# Define a strict schema\n",
    "strict_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),  # Not nullable\n",
    "    StructField(\"name\", StringType(), False),  # Not nullable\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Valid data\n",
    "valid_data = [(1, \"Alice\", 75000.0), (2, \"Bob\", 85000.0)]\n",
    "df_valid = spark.createDataFrame(valid_data, strict_schema)\n",
    "print(\"Valid data with strict schema:\")\n",
    "df_valid.show()\n",
    "\n",
    "# Try to create DataFrame with invalid data (this will work but may cause issues later)\n",
    "try:\n",
    "    invalid_data = [(1, \"Alice\", 75000.0), (2, None, 85000.0)]  # None in non-nullable field\n",
    "    df_invalid = spark.createDataFrame(invalid_data, strict_schema)\n",
    "    print(\"DataFrame created with invalid data:\")\n",
    "    df_invalid.show()  # This might fail or show unexpected results\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Schema comparison\n",
    "print(\"\\n=== SCHEMA COMPARISON ===\")\n",
    "schema1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"id\", LongType(), True),  # Different type\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schema 1:\", schema1.simpleString())\n",
    "print(\"Schema 2:\", schema2.simpleString())\n",
    "print(\"Are schemas equal?\", schema1 == schema2)\n",
    "\n",
    "# Working with nullable vs non-nullable fields\n",
    "print(\"\\n=== NULLABLE VS NON-NULLABLE ===\")\n",
    "nullable_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),    # Nullable\n",
    "    StructField(\"name\", StringType(), False)   # Non-nullable\n",
    "])\n",
    "\n",
    "print(\"Nullable schema:\", nullable_schema.simpleString())\n",
    "for field in nullable_schema.fields:\n",
    "    print(f\"Field '{field.name}': nullable={field.nullable}, type={field.dataType}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Practice Exercises\n",
    "\n",
    "Complete these exercises to practice schema specification and RDD operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise data - CSV-like format\n",
    "exercise_data = [\n",
    "    \"1,John Doe,Software Engineer,75000,2020-01-15,Python;Java;SQL\",\n",
    "    \"2,Jane Smith,Data Scientist,85000,2019-06-20,Python;R;Machine Learning\",\n",
    "    \"3,Bob Johnson,DevOps Engineer,80000,2021-03-10,Docker;Kubernetes;AWS\",\n",
    "    \"4,Alice Brown,Product Manager,90000,2018-09-05,Agile;Scrum;Analytics\"\n",
    "]\n",
    "\n",
    "# Create RDD from the data\n",
    "exercise_rdd = sc.parallelize(exercise_data)\n",
    "\n",
    "print(\"=== EXERCISE 1: Parse RDD and Create DataFrame ===\")\n",
    "print(\"Raw data:\")\n",
    "for line in exercise_data:\n",
    "    print(line)\n",
    "\n",
    "# TODO: Complete this exercise\n",
    "print(\"\\nTODO: Parse the RDD and create a DataFrame\")\n",
    "print(\"1. Create a function to parse each line\")\n",
    "print(\"2. Split by comma and handle the skills field (split by semicolon)\")\n",
    "print(\"3. Convert to appropriate data types\")\n",
    "print(\"4. Create DataFrame with inferred schema\")\n",
    "\n",
    "# Your code here:\n",
    "# def parse_employee_line(line):\n",
    "#     # Your parsing logic here\n",
    "#     pass\n",
    "\n",
    "# parsed_rdd = exercise_rdd.map(parse_employee_line)\n",
    "# df_exercise1 = spark.createDataFrame(parsed_rdd, [\"id\", \"name\", \"title\", \"salary\", \"hire_date\", \"skills\"])\n",
    "\n",
    "print(\"\\n=== EXERCISE 2: Define Custom Schema ===\")\n",
    "print(\"TODO: Define a custom schema for the employee data\")\n",
    "print(\"Requirements:\")\n",
    "print(\"- id: Integer, not nullable\")\n",
    "print(\"- name: String, not nullable\")\n",
    "print(\"- title: String, nullable\")\n",
    "print(\"- salary: Double, nullable\")\n",
    "print(\"- hire_date: Date, nullable\")\n",
    "print(\"- skills: Array of Strings, nullable\")\n",
    "\n",
    "# Your schema definition here:\n",
    "# employee_schema = StructType([\n",
    "#     # Your schema fields here\n",
    "# ])\n",
    "\n",
    "print(\"\\n=== EXERCISE 3: Create DataFrame with Custom Schema ===\")\n",
    "print(\"TODO: Create DataFrame using the custom schema you defined\")\n",
    "print(\"Handle the date conversion properly\")\n",
    "\n",
    "# Your code here:\n",
    "# df_exercise3 = spark.createDataFrame(parsed_rdd, employee_schema)\n",
    "\n",
    "print(\"\\n=== EXERCISE 4: Schema Validation ===\")\n",
    "print(\"TODO: Create a function to validate if a DataFrame matches expected schema\")\n",
    "\n",
    "# def validate_schema(df, expected_schema):\n",
    "#     # Your validation logic here\n",
    "#     pass\n",
    "\n",
    "print(\"\\n=== EXERCISE 5: Complex Nested Schema ===\")\n",
    "print(\"TODO: Create a nested schema for employee data with:\")\n",
    "print(\"- personal_info: struct with name, email\")\n",
    "print(\"- job_info: struct with title, salary, hire_date\")\n",
    "print(\"- skills: array of strings\")\n",
    "print(\"- certifications: array of structs with name and date\")\n",
    "\n",
    "# Your nested schema here:\n",
    "# nested_employee_schema = StructType([\n",
    "#     # Your nested schema definition here\n",
    "# ])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
