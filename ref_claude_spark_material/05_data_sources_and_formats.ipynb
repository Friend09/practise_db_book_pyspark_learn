{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Data Sources and File Formats - Practice Notebook\n",
    "\n",
    "This notebook covers reading and writing data in various formats, exploring Spark's built-in data sources.\n",
    "\n",
    "## Learning Objectives\n",
    "- Read and write CSV, JSON, Parquet files\n",
    "- Understand file format options and configurations\n",
    "- Work with different data sources\n",
    "- Handle schema evolution and data quality issues\n",
    "- Optimize file formats for performance\n",
    "\n",
    "## Sections\n",
    "1. **CSV Files - Reading and Writing**\n",
    "2. **JSON Files - Handling Semi-structured Data**\n",
    "3. **Parquet Files - Columnar Storage**\n",
    "4. **File Format Comparison**\n",
    "5. **Advanced Data Source Options**\n",
    "6. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37047b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Data Sources and Formats\").getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "sample_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\", [\"Python\", \"SQL\"]),\n",
    "    (2, \"Bob\", \"Sales\", 65000, \"2019-03-20\", [\"CRM\", \"Excel\"]),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000, \"2018-06-10\", [\"Java\", \"Scala\"]),\n",
    "    (4, \"Diana\", \"Marketing\", 70000, \"2021-02-28\", [\"Analytics\", \"Design\"]),\n",
    "    (5, \"Eve\", \"Sales\", 68000, \"2017-11-05\", [\"Negotiation\", \"Presentation\"])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sample_data, [\"id\", \"name\", \"department\", \"salary\", \"hire_date\", \"skills\"])\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(\"../data/formats\", exist_ok=True)\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. CSV Files - Reading and Writing\n",
    "\n",
    "CSV is one of the most common data formats. Let's explore various CSV options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing CSV files\n",
    "print(\"=== WRITING CSV FILES ===\")\n",
    "\n",
    "# Basic CSV write\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"../data/formats/employees.csv\")\n",
    "print(\"Basic CSV file written\")\n",
    "\n",
    "# CSV with custom options\n",
    "df.write.mode(\"overwrite\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"delimiter\", \"|\") \\\n",
    "  .option(\"quote\", '\"') \\\n",
    "  .option(\"escape\", \"\\\\\") \\\n",
    "  .csv(\"../data/formats/employees_custom.csv\")\n",
    "print(\"Custom CSV file written\")\n",
    "\n",
    "# Reading CSV files\n",
    "print(\"\\n=== READING CSV FILES ===\")\n",
    "\n",
    "# Basic CSV read\n",
    "df_csv_basic = spark.read.option(\"header\", \"true\").csv(\"../data/formats/employees.csv\")\n",
    "print(\"Basic CSV read:\")\n",
    "df_csv_basic.show()\n",
    "df_csv_basic.printSchema()\n",
    "\n",
    "# CSV read with schema inference\n",
    "df_csv_infer = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"../data/formats/employees.csv\")\n",
    "print(\"\\nCSV read with schema inference:\")\n",
    "df_csv_infer.show()\n",
    "df_csv_infer.printSchema()\n",
    "\n",
    "# CSV read with custom delimiter\n",
    "df_csv_custom = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"|\").csv(\"../data/formats/employees_custom.csv\")\n",
    "print(\"\\nCSV read with custom delimiter:\")\n",
    "df_csv_custom.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. JSON Files - Handling Semi-structured Data\n",
    "\n",
    "JSON is perfect for semi-structured data with nested fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb449283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing JSON files\n",
    "print(\"=== WRITING JSON FILES ===\")\n",
    "\n",
    "# Basic JSON write\n",
    "df.write.mode(\"overwrite\").json(\"../data/formats/employees.json\")\n",
    "print(\"JSON file written\")\n",
    "\n",
    "# Reading JSON files\n",
    "print(\"\\n=== READING JSON FILES ===\")\n",
    "\n",
    "df_json = spark.read.json(\"../data/formats/employees.json\")\n",
    "print(\"JSON read:\")\n",
    "df_json.show()\n",
    "df_json.printSchema()\n",
    "\n",
    "# Create more complex JSON data\n",
    "complex_data = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"personal\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@company.com\"},\n",
    "        \"job\": {\"title\": \"Engineer\", \"salary\": 75000, \"department\": \"Engineering\"},\n",
    "        \"skills\": [\"Python\", \"SQL\", \"Spark\"],\n",
    "        \"projects\": [\n",
    "            {\"name\": \"Project A\", \"status\": \"completed\"},\n",
    "            {\"name\": \"Project B\", \"status\": \"in_progress\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"personal\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@company.com\"},\n",
    "        \"job\": {\"title\": \"Analyst\", \"salary\": 65000, \"department\": \"Sales\"},\n",
    "        \"skills\": [\"Excel\", \"PowerBI\"],\n",
    "        \"projects\": [\n",
    "            {\"name\": \"Project C\", \"status\": \"completed\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame and write\n",
    "df_complex = spark.createDataFrame(complex_data)\n",
    "df_complex.write.mode(\"overwrite\").json(\"../data/formats/employees_complex.json\")\n",
    "\n",
    "# Read complex JSON\n",
    "df_complex_read = spark.read.json(\"../data/formats/employees_complex.json\")\n",
    "print(\"\\nComplex JSON structure:\")\n",
    "df_complex_read.show(truncate=False)\n",
    "df_complex_read.printSchema()\n",
    "\n",
    "# Access nested fields\n",
    "print(\"\\nAccessing nested fields:\")\n",
    "df_complex_read.select(\"id\", \"personal.name\", \"job.title\", \"job.salary\").show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Parquet Files - Columnar Storage\n",
    "\n",
    "Parquet is the preferred format for big data analytics due to its efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Parquet files\n",
    "print(\"=== WRITING PARQUET FILES ===\")\n",
    "\n",
    "# Basic Parquet write\n",
    "df.write.mode(\"overwrite\").parquet(\"../data/formats/employees.parquet\")\n",
    "print(\"Parquet file written\")\n",
    "\n",
    "# Parquet with partitioning\n",
    "df.write.mode(\"overwrite\").partitionBy(\"department\").parquet(\"../data/formats/employees_partitioned.parquet\")\n",
    "print(\"Partitioned Parquet file written\")\n",
    "\n",
    "# Reading Parquet files\n",
    "print(\"\\n=== READING PARQUET FILES ===\")\n",
    "\n",
    "df_parquet = spark.read.parquet(\"../data/formats/employees.parquet\")\n",
    "print(\"Parquet read:\")\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()\n",
    "\n",
    "# Read partitioned Parquet\n",
    "df_partitioned = spark.read.parquet(\"../data/formats/employees_partitioned.parquet\")\n",
    "print(\"\\nPartitioned Parquet read:\")\n",
    "df_partitioned.show()\n",
    "\n",
    "# Parquet preserves schema perfectly\n",
    "print(\"\\nParquet schema preservation:\")\n",
    "print(\"Original schema:\", df.schema.simpleString())\n",
    "print(\"Parquet schema:\", df_parquet.schema.simpleString())\n",
    "print(\"Schemas match:\", df.schema == df_parquet.schema)\n",
    "\n",
    "# Parquet with compression\n",
    "df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"../data/formats/employees_snappy.parquet\")\n",
    "df.write.mode(\"overwrite\").option(\"compression\", \"gzip\").parquet(\"../data/formats/employees_gzip.parquet\")\n",
    "print(\"\\nParquet files with different compression written\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Practice Exercises\n",
    "\n",
    "Complete these exercises to practice working with different data formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for exercises\n",
    "sales_data = [\n",
    "    (1, \"2024-01-15\", \"Product A\", 100, 25.50, \"Electronics\"),\n",
    "    (2, \"2024-01-16\", \"Product B\", 50, 15.75, \"Books\"),\n",
    "    (3, \"2024-01-17\", \"Product C\", 75, 30.00, \"Electronics\"),\n",
    "    (4, \"2024-01-18\", \"Product D\", 200, 5.25, \"Books\"),\n",
    "    (5, \"2024-01-19\", \"Product E\", 25, 45.00, \"Electronics\")\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"order_id\", \"date\", \"product\", \"quantity\", \"price\", \"category\"])\n",
    "print(\"Sales data for exercises:\")\n",
    "sales_df.show()\n",
    "\n",
    "print(\"\\n=== EXERCISE 1: CSV Operations ===\")\n",
    "print(\"TODO: Write the sales_df to CSV with the following requirements:\")\n",
    "print(\"1. Include header\")\n",
    "print(\"2. Use semicolon as delimiter\")\n",
    "print(\"3. Write to '../data/formats/sales_exercise.csv'\")\n",
    "print(\"4. Read it back and verify the data\")\n",
    "\n",
    "# Your code here:\n",
    "# sales_df.write...\n",
    "\n",
    "print(\"\\n=== EXERCISE 2: JSON with Nested Structure ===\")\n",
    "print(\"TODO: Transform the sales data to have nested structure:\")\n",
    "print(\"1. Create 'order_info' struct with order_id and date\")\n",
    "print(\"2. Create 'product_info' struct with product, quantity, price\")\n",
    "print(\"3. Keep category as top-level field\")\n",
    "print(\"4. Write to JSON and read back\")\n",
    "\n",
    "# Your code here:\n",
    "# nested_sales = sales_df.select(...)\n",
    "\n",
    "print(\"\\n=== EXERCISE 3: Parquet with Partitioning ===\")\n",
    "print(\"TODO: Write sales data to Parquet with partitioning:\")\n",
    "print(\"1. Partition by 'category'\")\n",
    "print(\"2. Use snappy compression\")\n",
    "print(\"3. Write to '../data/formats/sales_partitioned.parquet'\")\n",
    "print(\"4. Read back and show partition information\")\n",
    "\n",
    "# Your code here:\n",
    "# sales_df.write...\n",
    "\n",
    "print(\"\\n=== EXERCISE 4: Format Comparison ===\")\n",
    "print(\"TODO: Compare file sizes and read performance:\")\n",
    "print(\"1. Write the same data to CSV, JSON, and Parquet\")\n",
    "print(\"2. Check file sizes (use os.path.getsize)\")\n",
    "print(\"3. Time the read operations\")\n",
    "print(\"4. Compare schemas after reading\")\n",
    "\n",
    "# Your code here:\n",
    "# import time\n",
    "# import os\n",
    "\n",
    "print(\"\\n=== EXERCISE 5: Schema Evolution ===\")\n",
    "print(\"TODO: Handle schema evolution scenario:\")\n",
    "print(\"1. Write original sales_df to Parquet\")\n",
    "print(\"2. Create new version with additional 'discount' column\")\n",
    "print(\"3. Write new version to same location with mergeSchema option\")\n",
    "print(\"4. Read back and handle missing values\")\n",
    "\n",
    "# Your code here:\n",
    "# sales_v2 = sales_df.withColumn(\"discount\", F.lit(0.0))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
