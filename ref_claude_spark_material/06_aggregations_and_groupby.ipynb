{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Aggregations and GroupBy Operations - Practice Notebook\n",
    "\n",
    "This notebook focuses on **Aggregate Functions** and advanced grouping operations in Spark SQL.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master built-in aggregate functions\n",
    "- Understand groupBy operations and patterns\n",
    "- Practice window functions for advanced analytics\n",
    "- Work with pivot tables and rollup operations\n",
    "- Handle null values in aggregations\n",
    "\n",
    "## Sections\n",
    "1. **Basic Aggregations**\n",
    "2. **GroupBy Operations**\n",
    "3. **Window Functions**\n",
    "4. **Advanced Grouping (Rollup, Cube, Pivot)**\n",
    "5. **Custom Aggregations**\n",
    "6. **Practice Exercises**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Aggregations and GroupBy\").getOrCreate()\n",
    "\n",
    "# Create comprehensive sample data\n",
    "sales_data = [\n",
    "    (1, \"2024-01-15\", \"Alice\", \"Electronics\", \"Laptop\", 2, 1200.00, \"North\"),\n",
    "    (2, \"2024-01-15\", \"Bob\", \"Books\", \"Python Guide\", 1, 45.00, \"South\"),\n",
    "    (3, \"2024-01-16\", \"Alice\", \"Electronics\", \"Mouse\", 3, 25.00, \"North\"),\n",
    "    (4, \"2024-01-16\", \"Charlie\", \"Electronics\", \"Keyboard\", 1, 75.00, \"East\"),\n",
    "    (5, \"2024-01-17\", \"Bob\", \"Books\", \"Data Science\", 2, 60.00, \"South\"),\n",
    "    (6, \"2024-01-17\", \"Diana\", \"Clothing\", \"Shirt\", 4, 30.00, \"West\"),\n",
    "    (7, \"2024-01-18\", \"Alice\", \"Electronics\", \"Monitor\", 1, 300.00, \"North\"),\n",
    "    (8, \"2024-01-18\", \"Eve\", \"Clothing\", \"Pants\", 2, 50.00, \"West\"),\n",
    "    (9, \"2024-01-19\", \"Charlie\", \"Books\", \"ML Handbook\", 1, 80.00, \"East\"),\n",
    "    (10, \"2024-01-19\", \"Diana\", \"Electronics\", \"Tablet\", 1, 400.00, \"West\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sales_data, [\n",
    "    \"order_id\", \"date\", \"customer\", \"category\", \"product\",\n",
    "    \"quantity\", \"price\", \"region\"\n",
    "])\n",
    "\n",
    "# Add calculated columns\n",
    "df = df.withColumn(\"total_amount\", df.quantity * df.price)\n",
    "df = df.withColumn(\"date\", F.to_date(df.date, \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"Sample sales data:\")\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Basic Aggregations\n",
    "\n",
    "Start with fundamental aggregate functions that work on the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic aggregate functions\n",
    "print(\"=== BASIC AGGREGATIONS ===\")\n",
    "\n",
    "# Single aggregations\n",
    "print(\"1. Basic statistics:\")\n",
    "df.agg(\n",
    "    F.count(\"*\").alias(\"total_orders\"),\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    F.min(\"total_amount\").alias(\"min_order\"),\n",
    "    F.max(\"total_amount\").alias(\"max_order\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n2. More aggregate functions:\")\n",
    "df.agg(\n",
    "    F.countDistinct(\"customer\").alias(\"unique_customers\"),\n",
    "    F.countDistinct(\"category\").alias(\"unique_categories\"),\n",
    "    F.stddev(\"total_amount\").alias(\"std_deviation\"),\n",
    "    F.variance(\"total_amount\").alias(\"variance\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n3. Statistical functions:\")\n",
    "df.agg(\n",
    "    F.percentile_approx(\"total_amount\", 0.5).alias(\"median\"),\n",
    "    F.percentile_approx(\"total_amount\", 0.25).alias(\"q1\"),\n",
    "    F.percentile_approx(\"total_amount\", 0.75).alias(\"q3\"),\n",
    "    F.skewness(\"total_amount\").alias(\"skewness\"),\n",
    "    F.kurtosis(\"total_amount\").alias(\"kurtosis\")\n",
    ").show()\n",
    "\n",
    "# String aggregations\n",
    "print(\"\\n4. String aggregations:\")\n",
    "df.agg(\n",
    "    F.collect_list(\"customer\").alias(\"all_customers\"),\n",
    "    F.collect_set(\"customer\").alias(\"unique_customers_list\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. GroupBy Operations\n",
    "\n",
    "Group data by one or more columns and apply aggregate functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy operations\n",
    "print(\"=== GROUPBY OPERATIONS ===\")\n",
    "\n",
    "print(\"1. Group by single column:\")\n",
    "df.groupBy(\"category\").agg(\n",
    "    F.count(\"*\").alias(\"order_count\"),\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n2. Group by multiple columns:\")\n",
    "df.groupBy(\"category\", \"region\").agg(\n",
    "    F.count(\"*\").alias(\"order_count\"),\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    ").orderBy(\"category\", \"region\").show()\n",
    "\n",
    "print(\"\\n3. Group by customer analysis:\")\n",
    "customer_stats = df.groupBy(\"customer\").agg(\n",
    "    F.count(\"*\").alias(\"order_count\"),\n",
    "    F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "    F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    F.countDistinct(\"category\").alias(\"categories_purchased\"),\n",
    "    F.max(\"total_amount\").alias(\"largest_order\")\n",
    ").orderBy(F.desc(\"total_spent\"))\n",
    "\n",
    "customer_stats.show()\n",
    "\n",
    "print(\"\\n4. Date-based grouping:\")\n",
    "df.groupBy(\"date\").agg(\n",
    "    F.count(\"*\").alias(\"daily_orders\"),\n",
    "    F.sum(\"total_amount\").alias(\"daily_revenue\")\n",
    ").orderBy(\"date\").show()\n",
    "\n",
    "print(\"\\n5. Advanced grouping with conditions:\")\n",
    "df.groupBy(\"region\").agg(\n",
    "    F.count(\"*\").alias(\"total_orders\"),\n",
    "    F.sum(F.when(F.col(\"category\") == \"Electronics\", F.col(\"total_amount\")).otherwise(0)).alias(\"electronics_revenue\"),\n",
    "    F.sum(F.when(F.col(\"category\") == \"Books\", F.col(\"total_amount\")).otherwise(0)).alias(\"books_revenue\"),\n",
    "    F.sum(F.when(F.col(\"category\") == \"Clothing\", F.col(\"total_amount\")).otherwise(0)).alias(\"clothing_revenue\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Window Functions\n",
    "\n",
    "Window functions perform calculations across a set of rows related to the current row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions\n",
    "print(\"=== WINDOW FUNCTIONS ===\")\n",
    "\n",
    "print(\"1. Ranking functions:\")\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(F.desc(\"total_amount\"))\n",
    "\n",
    "df.select(\n",
    "    \"order_id\", \"customer\", \"category\", \"total_amount\",\n",
    "    F.row_number().over(window_spec).alias(\"row_number\"),\n",
    "    F.rank().over(window_spec).alias(\"rank\"),\n",
    "    F.dense_rank().over(window_spec).alias(\"dense_rank\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n2. Aggregate window functions:\")\n",
    "window_category = Window.partitionBy(\"category\")\n",
    "window_customer = Window.partitionBy(\"customer\").orderBy(\"date\")\n",
    "\n",
    "df.select(\n",
    "    \"order_id\", \"customer\", \"category\", \"total_amount\",\n",
    "    F.sum(\"total_amount\").over(window_category).alias(\"category_total\"),\n",
    "    F.avg(\"total_amount\").over(window_category).alias(\"category_avg\"),\n",
    "    F.count(\"*\").over(window_category).alias(\"category_count\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n3. Running totals and moving averages:\")\n",
    "df.select(\n",
    "    \"order_id\", \"customer\", \"date\", \"total_amount\",\n",
    "    F.sum(\"total_amount\").over(window_customer.rowsBetween(Window.unboundedPreceding, Window.currentRow)).alias(\"running_total\"),\n",
    "    F.avg(\"total_amount\").over(window_customer.rowsBetween(-1, 1)).alias(\"moving_avg_3\")\n",
    ").orderBy(\"customer\", \"date\").show()\n",
    "\n",
    "print(\"\\n4. Lag and Lead functions:\")\n",
    "df.select(\n",
    "    \"order_id\", \"customer\", \"date\", \"total_amount\",\n",
    "    F.lag(\"total_amount\", 1).over(window_customer).alias(\"prev_order_amount\"),\n",
    "    F.lead(\"total_amount\", 1).over(window_customer).alias(\"next_order_amount\")\n",
    ").orderBy(\"customer\", \"date\").show()\n",
    "\n",
    "print(\"\\n5. Percentile functions:\")\n",
    "window_all = Window.orderBy(\"total_amount\")\n",
    "\n",
    "df.select(\n",
    "    \"order_id\", \"customer\", \"total_amount\",\n",
    "    F.percent_rank().over(window_all).alias(\"percent_rank\"),\n",
    "    F.ntile(4).over(window_all).alias(\"quartile\")\n",
    ").orderBy(\"total_amount\").show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Practice Exercises\n",
    "\n",
    "Complete these exercises to master aggregations and groupBy operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional data for exercises\n",
    "employee_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\", \"Manager\"),\n",
    "    (2, \"Bob\", \"Sales\", 65000, \"2019-03-20\", \"Associate\"),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000, \"2018-06-10\", \"Senior\"),\n",
    "    (4, \"Diana\", \"Marketing\", 70000, \"2021-02-28\", \"Manager\"),\n",
    "    (5, \"Eve\", \"Sales\", 68000, \"2017-11-05\", \"Senior\"),\n",
    "    (6, \"Frank\", \"Engineering\", 82000, \"2020-09-12\", \"Senior\"),\n",
    "    (7, \"Grace\", \"Marketing\", 72000, \"2019-07-01\", \"Associate\"),\n",
    "    (8, \"Henry\", \"Sales\", 63000, \"2021-04-15\", \"Associate\")\n",
    "]\n",
    "\n",
    "emp_df = spark.createDataFrame(employee_data, [\"id\", \"name\", \"department\", \"salary\", \"hire_date\", \"level\"])\n",
    "emp_df = emp_df.withColumn(\"hire_date\", F.to_date(emp_df.hire_date, \"yyyy-MM-dd\"))\n",
    "emp_df = emp_df.withColumn(\"years_of_service\", F.datediff(F.current_date(), emp_df.hire_date) / 365.25)\n",
    "\n",
    "print(\"Employee data for exercises:\")\n",
    "emp_df.show()\n",
    "\n",
    "print(\"\\n=== EXERCISE 1: Department Analysis ===\")\n",
    "print(\"TODO: Create a department analysis report with:\")\n",
    "print(\"1. Number of employees per department\")\n",
    "print(\"2. Average salary per department\")\n",
    "print(\"3. Min and max salary per department\")\n",
    "print(\"4. Standard deviation of salaries per department\")\n",
    "\n",
    "# Your code here:\n",
    "# dept_analysis = emp_df.groupBy(\"department\").agg(...)\n",
    "\n",
    "print(\"\\n=== EXERCISE 2: Salary Bands ===\")\n",
    "print(\"TODO: Create salary bands and analyze distribution:\")\n",
    "print(\"1. Create bands: <60k, 60-70k, 70-80k, >80k\")\n",
    "print(\"2. Count employees in each band\")\n",
    "print(\"3. Show average years of service per band\")\n",
    "\n",
    "# Your code here:\n",
    "# salary_bands = emp_df.withColumn(\"salary_band\", ...)\n",
    "\n",
    "print(\"\\n=== EXERCISE 3: Window Functions ===\")\n",
    "print(\"TODO: Use window functions to:\")\n",
    "print(\"1. Rank employees by salary within each department\")\n",
    "print(\"2. Calculate running average salary by hire date\")\n",
    "print(\"3. Find salary percentile for each employee\")\n",
    "\n",
    "# Your code here:\n",
    "# window_dept = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "\n",
    "print(\"\\n=== EXERCISE 4: Advanced Grouping ===\")\n",
    "print(\"TODO: Create a pivot table showing:\")\n",
    "print(\"1. Departments as rows\")\n",
    "print(\"2. Levels as columns\")\n",
    "print(\"3. Average salary as values\")\n",
    "\n",
    "# Your code here:\n",
    "# pivot_table = emp_df.groupBy(\"department\").pivot(\"level\").agg(...)\n",
    "\n",
    "print(\"\\n=== EXERCISE 5: Time-based Analysis ===\")\n",
    "print(\"TODO: Analyze hiring trends:\")\n",
    "print(\"1. Group by hire year and count employees\")\n",
    "print(\"2. Calculate average salary by hire year\")\n",
    "print(\"3. Show cumulative hiring count over time\")\n",
    "\n",
    "# Your code here:\n",
    "# emp_with_year = emp_df.withColumn(\"hire_year\", F.year(\"hire_date\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
