{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Live Tables (DLT) and dbt Medallion Architecture Learning Guide\n",
    "\n",
    "Welcome to your DLT and dbt learning repository! This notebook will guide you through implementing the Medallion Architecture using Delta Live Tables for Bronze and Silver layers, and dbt for the Gold layer, all within Databricks.\n",
    "\n",
    "## 1. Setting up your Databricks Environment\n",
    "\n",
    "Before you begin, ensure you have access to a Databricks workspace. You will need to upload the DLT pipeline scripts (`dlt_pipelines/bronze_layer.py` and `dlt_pipelines/silver_layer.py`) to your Databricks workspace and create a DLT pipeline. You will also need to configure your dbt project to connect to your Databricks workspace.\n",
    "\n",
    "**Exercise 1.1**: Upload the `dlt_pipelines` scripts to Databricks and create a DLT pipeline. Run the DLT pipeline to ingest and transform the data into Bronze and Silver Delta tables.\n",
    "\n",
    "**Exercise 1.2**: Ensure your `~/.dbt/profiles.yml` is configured correctly to connect to your Databricks workspace. You can test the connection by running `dbt debug` in your terminal from the `dbt_models` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bronze Layer with DLT: Raw Data Ingestion\n",
    "\n",
    "The Bronze layer in DLT ingests raw data. Our `dlt_pipelines/bronze_layer.py` script reads the `sales_data.csv` and creates a `bronze_sales` Delta table. After running your DLT pipeline, you can query this table.\n",
    "\n",
    "**Exercise 2.1**: After your DLT pipeline has run successfully, use a SQL cell in Databricks to query the `bronze_sales` table. Inspect its schema and a few rows.\n",
    "\n",
    "```sql\n",
    "SELECT * FROM bronze_sales LIMIT 10;\n",
    "DESCRIBE bronze_sales;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Silver Layer with DLT: Cleaned and Conformed Data\n",
    "\n",
    "The Silver layer in DLT transforms the raw data. Our `dlt_pipelines/silver_layer.py` script reads from `bronze_sales`, calculates `total_price`, and adds a `processing_timestamp`, creating a `silver_sales` Delta table.\n",
    "\n",
    "**Exercise 3.1**: After your DLT pipeline has run successfully, query the `silver_sales` table in Databricks. Verify the new columns and data quality.\n",
    "\n",
    "```sql\n",
    "SELECT * FROM silver_sales LIMIT 10;\n",
    "DESCRIBE silver_sales;\n",
    "```\n",
    "\n",
    "**Exercise 3.2**: Modify the `silver_layer.py` script to add another transformation, for example, converting the `product_name` to uppercase. Rerun your DLT pipeline and observe the changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gold Layer with dbt: Aggregated Data for Analytics\n",
    "\n",
    "The Gold layer is built using dbt models on top of your Silver layer Delta tables. Our `dbt_models/models/gold_sales_summary.sql` model aggregates the `silver_sales` data to calculate `total_revenue` per product and city.\n",
    "\n",
    "**Exercise 4.1**: Navigate to the `dbt_models` directory in your terminal. Run your dbt models to create the Gold layer table.\n",
    "\n",
    "```bash\n",
    "cd dbt_models\n",
    "dbt run\n",
    "```\n",
    "\n",
    "**Exercise 4.2**: After `dbt run` completes, query the `gold_sales_summary` table in Databricks. Verify the aggregated results.\n",
    "\n",
    "```sql\n",
    "SELECT * FROM gold_sales_summary LIMIT 10;\n",
    "```\n",
    "\n",
    "**Exercise 4.3**: Create a new dbt model in the `dbt_models/models` directory that calculates the total revenue per customer. Run `dbt run` again and query your new Gold layer table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Experiment with different DLT features like expectations and incremental tables.\n",
    "- Explore more advanced dbt concepts like tests, documentation, and macros.\n",
    "- Integrate your DLT pipelines and dbt models into a CI/CD workflow.\n",
    "\n",
    "Happy Data Engineering with DLT and dbt!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

