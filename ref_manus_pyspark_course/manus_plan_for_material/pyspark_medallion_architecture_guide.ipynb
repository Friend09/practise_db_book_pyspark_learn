{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Medallion Architecture Learning Guide\n",
    "\n",
    "Welcome to your PySpark learning repository! This notebook will guide you through the core concepts of PySpark for data engineering, focusing on the Medallion Architecture (Bronze, Silver, Gold layers). You'll learn how to set up your local environment, ingest raw data, transform it, and aggregate it for analytical purposes.\n",
    "\n",
    "## 1. Setting up your Spark Session\n",
    "\n",
    "Before we begin, let's ensure your Spark session is correctly configured. We've provided a utility function in `conf/spark_session_config.py` to help with this. Run the following Python code to initialize your Spark session.\n",
    "\n",
    "**Exercise 1.1**: Run the cell below to create a SparkSession and print its version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path to import custom modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from conf.spark_session_config import get_spark_session\n",
    "\n",
    "spark = get_spark_session(app_name=\"MedallionArchitectureGuide\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bronze Layer: Raw Data Ingestion\n",
    "\n",
    "The Bronze layer is where raw data is ingested as-is from source systems. We've already generated some sample sales data for you in `data/raw/sales_data.csv`. The `scripts/bronze_ingestion.py` script reads this CSV and saves it as a Delta table in `data/bronze/sales`.\n",
    "\n",
    "**Exercise 2.1**: Run the `bronze_ingestion.py` script from your terminal (or a new cell if you prefer, but it's designed to be run as a script). Then, use PySpark to read the ingested Bronze table and display its schema and a few rows.\n",
    "\n",
    "```bash\n",
    "# From your project root directory in the terminal:\n",
    "# PYTHONPATH=$(pwd) python3 scripts/bronze_ingestion.py\n",
    "```\n",
    "\n",
    "**Exercise 2.2**: Read the Bronze table using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_df = spark.read.format(\"delta\").load(\"../data/bronze/sales\")\n",
    "bronze_df.printSchema()\n",
    "bronze_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Silver Layer: Cleaned and Conformed Data\n",
    "\n",
    "The Silver layer contains cleaned, conformed, and enriched data. In our example, we'll calculate the `total_price` and add a `processing_timestamp`. The `scripts/silver_transformation.py` script performs this transformation.\n",
    "\n",
    "**Exercise 3.1**: Run the `silver_transformation.py` script from your terminal. Then, read the Silver table and inspect its schema and data.\n",
    "\n",
    "```bash\n",
    "# From your project root directory in the terminal:\n",
    "# PYTHONPATH=$(pwd) python3 scripts/silver_transformation.py\n",
    "```\n",
    "\n",
    "**Exercise 3.2**: Read the Silver table using PySpark and display its content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_df = spark.read.format(\"delta\").load(\"../data/silver/sales\")\n",
    "silver_df.printSchema()\n",
    "silver_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3**: Try to perform an additional transformation. For example, filter the data to only include sales where `quantity` is greater than 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 3.3\n",
    "# filtered_silver_df = silver_df.filter(silver_df.quantity > 1)\n",
    "# filtered_silver_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gold Layer: Aggregated Data for Analytics\n",
    "\n",
    "The Gold layer is optimized for analytics and reporting. Here, we'll aggregate the sales data to get the total revenue per product and city. The `scripts/gold_aggregation.py` script handles this.\n",
    "\n",
    "**Exercise 4.1**: Run the `gold_aggregation.py` script from your terminal. Then, read the Gold table and examine the aggregated results.\n",
    "\n",
    "```bash\n",
    "# From your project root directory in the terminal:\n",
    "# PYTHONPATH=$(pwd) python3 scripts/gold_aggregation.py\n",
    "```\n",
    "\n",
    "**Exercise 4.2**: Read the Gold table using PySpark and display its content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = spark.read.format(\"delta\").load(\"../data/gold/sales_summary\")\n",
    "gold_df.printSchema()\n",
    "gold_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.3**: Use Spark SQL to query the Gold table. For example, find the top 5 products by total revenue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 4.3\n",
    "# spark.sql(\"SELECT * FROM sales_summary ORDER BY total_revenue DESC LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleaning Up\n",
    "\n",
    "After you're done practicing, it's good practice to stop your SparkSession.\n",
    "\n",
    "**Exercise 5.1**: Stop the SparkSession.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"SparkSession stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Experiment with different data transformations and aggregations.\n",
    "- Try ingesting data from other formats (e.g., JSON, Parquet) into the Bronze layer.\n",
    "- Explore more advanced PySpark features like UDFs, window functions, and structured streaming.\n",
    "- Apply these concepts to your Databricks projects!\n",
    "\n",
    "Happy PySparking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

