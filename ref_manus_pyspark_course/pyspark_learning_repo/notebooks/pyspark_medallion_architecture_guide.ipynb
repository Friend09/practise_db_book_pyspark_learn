{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Medallion Architecture Learning Guide\n",
    "\n",
    "Welcome to your PySpark learning repository! This notebook will guide you through the core concepts of PySpark for data engineering, focusing on the Medallion Architecture (Bronze, Silver, Gold layers). You'll learn how to set up your local environment, ingest raw data, transform it, and aggregate it for analytical purposes.\n",
    "\n",
    "## 1. Setting up your Spark Session\n",
    "\n",
    "Before we begin, let's ensure your Spark session is correctly configured. We've provided a utility function in `conf/spark_session_config.py` to help with this. Run the following Python code to initialize your Spark session.\n",
    "\n",
    "**Exercise 1.1**: Run the cell below to create a SparkSession and print its version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.0\n",
      "Spark Context available: True\n",
      "Project root: /Users/vamsi_mbmax/Developer/VAM_Documents/01_vam_PROJECTS/LEARNING/proj_Databases/dev_proj_Databases/practise_db_book_pyspark_learn/ref_manus_pyspark_course/pyspark_learning_repo\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules and create SparkSession with proper configuration\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add the project root to the Python path to import custom modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from conf.spark_session_config import get_spark_session\n",
    "\n",
    "# Create SparkSession using our configuration utility\n",
    "spark = get_spark_session(app_name=\"MedallionArchitectureGuide\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Context available: {spark.sparkContext is not None}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bronze Layer: Raw Data Ingestion\n",
    "\n",
    "The Bronze layer is where raw data is ingested as-is from source systems. We've already generated some sample sales data for you in `data/raw/sales_data.csv`. The `scripts/bronze_ingestion.py` script reads this CSV and saves it as a Delta table in `data/bronze/sales`.\n",
    "\n",
    "**Exercise 2.1**: Run the `bronze_ingestion.py` script from your terminal (or a new cell if you prefer, but it's designed to be run as a script). Then, use PySpark to read the ingested Bronze table and display its schema and a few rows.\n",
    "\n",
    "```bash\n",
    "# From your project root directory in the terminal:\n",
    "# PYTHONPATH=$(pwd) python3 scripts/bronze_ingestion.py\n",
    "```\n",
    "\n",
    "**Exercise 2.2**: Read the Bronze table using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw data from /Users/vamsi_mbmax/Developer/VAM_Documents/01_vam_PROJECTS/LEARNING/proj_Databases/dev_proj_Databases/practise_db_book_pyspark_learn/data/raw/sales_data.csv...\n",
      "Data schema:\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      "\n",
      "Number of records: 1000\n",
      "Writing data to Bronze layer at data/bronze/sales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer ingestion complete.\n",
      "Sample data:\n",
      "+--------------+------------+-------------+-----------+--------+------+----------------+\n",
      "|transaction_id|product_name|customer_name|       city|quantity| price|transaction_date|\n",
      "+--------------+------------+-------------+-----------+--------+------+----------------+\n",
      "|             1|    Keyboard|  Customer_49|   New York|       4|678.03|      2024-06-30|\n",
      "|             2|      Laptop|  Customer_37|Los Angeles|       3|637.83|      2024-07-05|\n",
      "|             3|      Laptop|  Customer_40|    Houston|       2|665.58|      2024-10-21|\n",
      "|             4|  Headphones|   Customer_3|Los Angeles|       4|222.11|      2024-11-20|\n",
      "|             5|       Mouse|  Customer_10|Los Angeles|       3|  87.2|      2024-07-25|\n",
      "+--------------+------------+-------------+-----------+--------+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "==================================================\n",
      "Bronze layer ingestion completed!\n",
      "==================================================\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      "\n",
      "+--------------+------------+-------------+-----------+--------+------+----------------+\n",
      "|transaction_id|product_name|customer_name|       city|quantity| price|transaction_date|\n",
      "+--------------+------------+-------------+-----------+--------+------+----------------+\n",
      "|             1|    Keyboard|  Customer_49|   New York|       4|678.03|      2024-06-30|\n",
      "|             2|      Laptop|  Customer_37|Los Angeles|       3|637.83|      2024-07-05|\n",
      "|             3|      Laptop|  Customer_40|    Houston|       2|665.58|      2024-10-21|\n",
      "|             4|  Headphones|   Customer_3|Los Angeles|       4|222.11|      2024-11-20|\n",
      "|             5|       Mouse|  Customer_10|Los Angeles|       3|  87.2|      2024-07-25|\n",
      "+--------------+------------+-------------+-----------+--------+------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Run the bronze ingestion script to create the bronze layer\n",
    "from scripts.bronze_ingestion import ingest_raw_sales_data\n",
    "\n",
    "# Ingest data from raw CSV to bronze layer (Parquet format)\n",
    "ingest_raw_sales_data(spark, input_path=\"/Users/vamsi_mbmax/Developer/VAM_Documents/01_vam_PROJECTS/LEARNING/proj_Databases/dev_proj_Databases/practise_db_book_pyspark_learn/data/raw/sales_data.csv\", output_path=\"data/bronze/sales\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Bronze layer ingestion completed!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Now read the bronze layer data to verify it was created correctly\n",
    "bronze_df = spark.read.parquet(\"data/bronze/sales\")\n",
    "bronze_df.printSchema()\n",
    "bronze_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Silver Layer: Cleaned and Conformed Data\n",
    "\n",
    "The Silver layer contains cleaned, conformed, and enriched data. In our example, we'll calculate the `total_price` and add a `processing_timestamp`. The `scripts/silver_transformation.py` script performs this transformation.\n",
    "\n",
    "**Exercise 3.1**: Run the `silver_transformation.py` script from your terminal. Then, read the Silver table and inspect its schema and data.\n",
    "\n",
    "```bash\n",
    "# From your project root directory in the terminal:\n",
    "# PYTHONPATH=$(pwd) python3 scripts/silver_transformation.py\n",
    "```\n",
    "\n",
    "**Exercise 3.2**: Read the Silver table using PySpark and display its content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the silver transformation script to create the silver layer\n",
    "from scripts.silver_transformation import transform_sales_data\n",
    "\n",
    "# Transform data from bronze to silver layer\n",
    "transform_sales_data(spark, input_path=\"data/bronze/sales\", output_path=\"data/silver/sales\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Silver layer transformation completed!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Now read the silver layer data to verify it was created correctly\n",
    "silver_df = spark.read.parquet(\"data/silver/sales\")\n",
    "silver_df.printSchema()\n",
    "silver_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3**: Try to perform an additional transformation. For example, filter the data to only include sales where `quantity` is greater than 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3: Filter the silver_df to show only sales where quantity > 1\n",
    "filtered_silver_df = silver_df.filter(silver_df.quantity > 1)\n",
    "\n",
    "print(f\"Records with quantity > 1: {filtered_silver_df.count()}\")\n",
    "print(\"Sample filtered data:\")\n",
    "filtered_silver_df.show(5)\n",
    "\n",
    "# Additional transformations you can try:\n",
    "# 1. Filter by date range\n",
    "recent_sales = silver_df.filter(F.col(\"transaction_date\") >= \"2024-06-01\")\n",
    "print(f\"\\nRecent sales (from June 2024): {recent_sales.count()}\")\n",
    "\n",
    "# 2. Filter by high-value transactions\n",
    "high_value_sales = silver_df.filter(F.col(\"total_price\") > 1000)\n",
    "print(f\"High-value sales (> $1000): {high_value_sales.count()}\")\n",
    "\n",
    "# 3. Group by product and show summary statistics\n",
    "product_summary = silver_df.groupBy(\"product_name\").agg(\n",
    "    F.count(\"transaction_id\").alias(\"total_transactions\"),\n",
    "    F.sum(\"total_price\").alias(\"total_revenue\"),\n",
    "    F.avg(\"total_price\").alias(\"avg_order_value\")\n",
    ").orderBy(F.col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"\\nProduct summary:\")\n",
    "product_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gold Layer: Aggregated Data for Analytics\n",
    "\n",
    "The Gold layer is optimized for analytics and reporting. Here, we'll aggregate the sales data to get the total revenue per product and city. The `scripts/gold_aggregation.py` script handles this.\n",
    "\n",
    "**Exercise 4.1**: Run the `gold_aggregation.py` script from your terminal. Then, read the Gold table and examine the aggregated results.\n",
    "\n",
    "```bash\n",
    "# From your project root directory in the terminal:\n",
    "# PYTHONPATH=$(pwd) python3 scripts/gold_aggregation.py\n",
    "```\n",
    "\n",
    "**Exercise 4.2**: Read the Gold table using PySpark and display its content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the gold aggregation script to create the gold layer\n",
    "from scripts.gold_aggregation import aggregate_sales_data\n",
    "\n",
    "# Aggregate data from silver to gold layer\n",
    "aggregate_sales_data(spark, input_path=\"data/silver/sales\", output_path=\"data/gold/sales_summary\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Gold layer aggregation completed!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Now read the gold layer data to verify it was created correctly\n",
    "gold_df = spark.read.parquet(\"data/gold/sales_summary\")\n",
    "gold_df.printSchema()\n",
    "gold_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.3**: Use Spark SQL to query the Gold table. For example, find the top 5 products by total revenue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.3: Use Spark SQL to query the Gold table\n",
    "# First, create a temporary view\n",
    "gold_df.createOrReplaceTempView(\"sales_summary\")\n",
    "\n",
    "# Query 1: Find the top 5 products by total revenue\n",
    "print(\"Top 5 products by total revenue:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT product_name, SUM(total_revenue) as total_product_revenue\n",
    "    FROM sales_summary\n",
    "    GROUP BY product_name\n",
    "    ORDER BY total_product_revenue DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "# Query 2: Find the top 5 cities by total revenue\n",
    "print(\"Top 5 cities by total revenue:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT city, SUM(total_revenue) as total_city_revenue\n",
    "    FROM sales_summary\n",
    "    GROUP BY city\n",
    "    ORDER BY total_city_revenue DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "# Query 3: Find the best performing product-city combinations\n",
    "print(\"Top 10 product-city combinations by revenue:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT product_name, city, total_revenue, total_transactions\n",
    "    FROM sales_summary\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "# Query 4: Summary statistics across all data\n",
    "print(\"Overall summary statistics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_combinations,\n",
    "        SUM(total_revenue) as grand_total_revenue,\n",
    "        AVG(total_revenue) as avg_revenue_per_combination,\n",
    "        SUM(total_transactions) as total_transactions,\n",
    "        SUM(total_quantity_sold) as total_quantity_sold\n",
    "    FROM sales_summary\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleaning Up\n",
    "\n",
    "After you're done practicing, it's good practice to stop your SparkSession.\n",
    "\n",
    "**Exercise 5.1**: Stop the SparkSession.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1: Stop the SparkSession\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped.\")\n",
    "\n",
    "# Summary of what we accomplished:\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEDALLION ARCHITECTURE PIPELINE COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ Bronze Layer: Raw data ingested from CSV to Parquet\")\n",
    "print(\"✅ Silver Layer: Data cleaned, enriched with calculated columns\")\n",
    "print(\"✅ Gold Layer: Data aggregated for analytics and reporting\")\n",
    "print(\"✅ SQL Queries: Performed analytics on the Gold layer\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Experiment with different data transformations and aggregations.\n",
    "- Try ingesting data from other formats (e.g., JSON, Parquet) into the Bronze layer.\n",
    "- Explore more advanced PySpark features like UDFs, window functions, and structured streaming.\n",
    "- Apply these concepts to your Databricks projects!\n",
    "\n",
    "Happy PySparking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
